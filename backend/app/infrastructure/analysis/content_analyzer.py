import os # osãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã®å…ˆé ­ã«ç§»å‹•

from typing import List, Dict, Any, Optional
from collections import Counter # Added for analyze_user_intent, _find_common_heading_patterns
import re # Added for analyze_user_intent, _find_common_heading_patterns
import numpy as np # â˜… Added for statistical analysis
import json # â˜… Added for JSON export
import datetime # â˜… Added for timestamp in exports
# ScrapedArticle ã‚’ serpapi_service ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ã“ã¨ã‚’æƒ³å®š
# å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹æˆã«ã‚ˆã£ã¦ã¯ã€å…±é€šã®å‹å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ãªã©ã«ç§»å‹•ã™ã‚‹ã“ã¨ã‚‚æ¤œè¨
from app.infrastructure.external_apis.serpapi_service import ScrapedArticle, SerpAnalysisResult # ScrapedArticleã«åŠ ãˆã¦SerpAnalysisResultã‚‚ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆã®ãŸã‚ï¼‰
import asyncio
from app.infrastructure.gcp_auth import setup_genai_client

class ContentAnalyzer:
    """
    ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã•ã‚ŒãŸè¤‡æ•°ã®è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€SEOæˆ¦ç•¥ã«å½¹ç«‹ã¤æ´å¯Ÿã‚’æä¾›ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚
    """
    
    @classmethod
    def quick_analyze(cls, scraped_articles: List[ScrapedArticle], output_file: str = None, language: str = "jp") -> Dict[str, Any]:
        """
        ãƒ¯ãƒ³ãƒ©ã‚¤ãƒŠãƒ¼ã§åˆ†æã‚’å®Ÿè¡Œã—ã€çµæœã‚’è¿”ã™ç°¡æ˜“ãƒ¡ã‚½ãƒƒãƒ‰
        
        Args:
            scraped_articles: åˆ†æå¯¾è±¡ã®è¨˜äº‹ãƒªã‚¹ãƒˆ
            output_file: ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€‚JSONãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›å…ˆ
            language: "jp" (æ—¥æœ¬èª) ã¾ãŸã¯ "en" (è‹±èª) ã¾ãŸã¯ "both" (ä¸¡æ–¹)
            
        Returns:
            åˆ†æçµæœã®è¾æ›¸
        """
        analyzer = cls(scraped_articles)
        results = analyzer.get_full_analysis()
        
        if output_file:
            analyzer.export_to_json(output_file, language)
            
        return results
    
    @classmethod 
    async def quick_analyze_with_ai(cls, scraped_articles: List[ScrapedArticle], output_file: str = None, language: str = "jp") -> Dict[str, Any]:
        """
        Gemini AIã‚’ä½¿ã£ãŸé«˜åº¦åˆ†æã‚’ãƒ¯ãƒ³ãƒ©ã‚¤ãƒŠãƒ¼ã§å®Ÿè¡Œ
        
        Args:
            scraped_articles: åˆ†æå¯¾è±¡ã®è¨˜äº‹ãƒªã‚¹ãƒˆ
            output_file: ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€‚JSONãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›å…ˆ
            language: "jp" (æ—¥æœ¬èª) ã¾ãŸã¯ "en" (è‹±èª) ã¾ãŸã¯ "both" (ä¸¡æ–¹)
            
        Returns:
            AIåˆ†æçµæœã®è¾æ›¸
        """
        analyzer = cls(scraped_articles)
        results = await analyzer.get_full_analysis_with_gemini()
        
        if output_file:
            analyzer.export_to_json(output_file, language)
            
        return results

    def __init__(self, scraped_articles: List[ScrapedArticle]):
        """
        ContentAnalyzerã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚

        Args:
            scraped_articles: åˆ†æå¯¾è±¡ã®ScrapedArticleã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆã€‚
        """
        self.analysis_results: Dict[str, Any] = {} # å…ˆã«åˆæœŸåŒ–

        if not scraped_articles:
            print("ContentAnalyzeråˆæœŸåŒ–: æ¸¡ã•ã‚ŒãŸè¨˜äº‹ãƒªã‚¹ãƒˆãŒç©ºã§ã™ã€‚åˆ†æçµæœã¯ç©ºã«ãªã‚Šã¾ã™ã€‚")
            self.articles: List[ScrapedArticle] = []
            return

        print(f"ContentAnalyzeråˆæœŸåŒ–: å…ƒã®è¨˜äº‹æ•° {len(scraped_articles)}ä»¶ã€‚æ–‡å­—æ•°0ã®è¨˜äº‹ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¾ã™...")
        
        original_count = len(scraped_articles)
        # char_countå±æ€§ã®å­˜åœ¨ã‚‚ç¢ºèª
        filtered_articles = [
            article for article in scraped_articles
            if hasattr(article, 'char_count') and isinstance(article.char_count, (int, float)) and article.char_count > 0
        ]
        
        filtered_out_count = original_count - len(filtered_articles)
        if filtered_out_count > 0:
            print(f"ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: {filtered_out_count}ä»¶ã®æ–‡å­—æ•°0ã¾ãŸã¯æ–‡å­—æ•°å–å¾—å¤±æ•—ã®è¨˜äº‹ã‚’é™¤å¤–ã—ã¾ã—ãŸã€‚")

        if not filtered_articles:
            print("è­¦å‘Š: ContentAnalyzer - ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®çµæœã€åˆ†æå¯¾è±¡ã¨ãªã‚‹æœ‰åŠ¹ãªè¨˜äº‹ãŒ0ä»¶ã§ã™ã€‚")
            self.articles: List[ScrapedArticle] = []
        else:
            self.articles: List[ScrapedArticle] = filtered_articles
            print(f"ContentAnalyzeråˆæœŸåŒ–å®Œäº†: åˆ†æå¯¾è±¡ã®è¨˜äº‹æ•° {len(self.articles)}ä»¶ã€‚")

    def _analyze_distribution(self, data: List[float], feature_name: str) -> Dict[str, Any]:
        """æ•°å€¤ãƒªã‚¹ãƒˆã®åˆ†å¸ƒã‚’åˆ†æã™ã‚‹å†…éƒ¨ãƒ¡ã‚½ãƒƒãƒ‰"""
        if not data:
            return {
                f"{feature_name}_stats": {"message": "ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®ãŸã‚åˆ†æã§ãã¾ã›ã‚“ã€‚"}
            }

        stats = {
            "å¹³å‡å€¤": float(np.mean(data)),
            "mean": float(np.mean(data)),
            "ä¸­å¤®å€¤": float(np.median(data)),
            "median": float(np.median(data)),
            "æ¨™æº–åå·®": float(np.std(data)),
            "std_dev": float(np.std(data)),
            "åˆ†æ•£": float(np.var(data)),
            "variance": float(np.var(data)),
            "æœ€å°å€¤": float(np.min(data)),
            "min": float(np.min(data)),
            "æœ€å¤§å€¤": float(np.max(data)),
            "max": float(np.max(data)),
            "ç¯„å›²ï¼ˆæœ€å¤§å€¤-æœ€å°å€¤ï¼‰": float(np.max(data) - np.min(data)),
            "range": float(np.max(data) - np.min(data)),
            "ç¬¬1å››åˆ†ä½æ•°ï¼ˆ25ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ï¼‰": float(np.percentile(data, 25)),
            "q1": float(np.percentile(data, 25)),
            "ç¬¬3å››åˆ†ä½æ•°ï¼ˆ75ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ï¼‰": float(np.percentile(data, 75)),
            "q3": float(np.percentile(data, 75)),
            "å››åˆ†ä½ç¯„å›²": float(np.percentile(data, 75) - np.percentile(data, 25)),
            "iqr": float(np.percentile(data, 75) - np.percentile(data, 25)),
            "ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«å€¤": {
                "10ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«": float(np.percentile(data, 10)),
                "10th": float(np.percentile(data, 10)),
                "90ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«": float(np.percentile(data, 90)),
                "90th": float(np.percentile(data, 90))
            },
            "percentiles": {
                "10th": float(np.percentile(data, 10)),
                "90th": float(np.percentile(data, 90))
            },
            "å¤–ã‚Œå€¤åˆ¤å®šåŸºæº–": {
                "å¤–ã‚Œå€¤ã®ä¸‹é™": float(np.percentile(data, 25) - 1.5 * (np.percentile(data, 75) - np.percentile(data, 25))),
                "lower_bound": float(np.percentile(data, 25) - 1.5 * (np.percentile(data, 75) - np.percentile(data, 25))),
                "å¤–ã‚Œå€¤ã®ä¸Šé™": float(np.percentile(data, 75) + 1.5 * (np.percentile(data, 75) - np.percentile(data, 25))),
                "upper_bound": float(np.percentile(data, 75) + 1.5 * (np.percentile(data, 75) - np.percentile(data, 25)))
            },
            "outlier_thresholds": {
                "lower_bound": float(np.percentile(data, 25) - 1.5 * (np.percentile(data, 75) - np.percentile(data, 25))),
                "upper_bound": float(np.percentile(data, 75) + 1.5 * (np.percentile(data, 75) - np.percentile(data, 25)))
            },
            "ãƒ‡ãƒ¼ã‚¿æ•°": len(data),
            "count": len(data),
            "çµ±è¨ˆã‚µãƒãƒªãƒ¼": f"ãƒ‡ãƒ¼ã‚¿æ•°{len(data)}ä»¶ã®çµ±è¨ˆ: å¹³å‡{float(np.mean(data)):.1f}, ä¸­å¤®å€¤{float(np.median(data)):.1f}, æœ€å°å€¤{float(np.min(data)):.1f}, æœ€å¤§å€¤{float(np.max(data)):.1f}",
            "summary_jp": f"ãƒ‡ãƒ¼ã‚¿æ•°{len(data)}ä»¶ã®çµ±è¨ˆ: å¹³å‡{float(np.mean(data)):.1f}, ä¸­å¤®å€¤{float(np.median(data)):.1f}, æœ€å°å€¤{float(np.min(data)):.1f}, æœ€å¤§å€¤{float(np.max(data)):.1f}"
        }
        return stats

    def analyze_basic_statistics(self) -> Dict[str, Any]:
        """è¨˜äº‹ç¾¤ã®åŸºæœ¬çš„ãªæ•°å€¤ç‰¹å¾´ã«é–¢ã™ã‚‹çµ±è¨ˆæƒ…å ±ã‚’åˆ†æã™ã‚‹"""
        print("åŸºæœ¬çµ±è¨ˆé‡ã®åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
        if not self.articles: # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œãªã®ã§ã€ã“ã“ã§ç©ºãªã‚‰æœ¬å½“ã«åˆ†æå¯¾è±¡ãŒãªã„
            print("ContentAnalyzer: åˆ†æå¯¾è±¡ã®è¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“ã€‚åŸºæœ¬çµ±è¨ˆåˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            self.analysis_results["basic_statistics"] = {"message": "è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚"}
            return self.analysis_results["basic_statistics"]

        # åˆ†æå¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        char_counts = [article.char_count for article in self.articles if hasattr(article, 'char_count')]
        image_counts = [article.image_count for article in self.articles if hasattr(article, 'image_count')]
        
        # heading_counts ã®è¨ˆç®—æ–¹æ³•ã‚’ä¿®æ­£: è¨˜äº‹å†…ã®å…¨Hã‚¿ã‚°ã®ç·æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        heading_counts = []
        for article in self.articles:
            if hasattr(article, 'headings') and article.headings: # None ã‚„ç©ºãƒªã‚¹ãƒˆã§ãªã„ã“ã¨ã‚’ç¢ºèª
                flat_headings = self._extract_headings_flat(article.headings)
                heading_counts.append(len(flat_headings))
            else:
                heading_counts.append(0)
        
        # â˜… æ–°ã—ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆé–¢é€£ã®çµ±è¨ˆ
        video_counts = [getattr(article, 'video_count', 0) for article in self.articles]
        table_counts = [getattr(article, 'table_count', 0) for article in self.articles]
        list_item_counts = [getattr(article, 'list_item_count', 0) for article in self.articles]
        external_link_counts = [getattr(article, 'external_link_count', 0) for article in self.articles]
        internal_link_counts = [getattr(article, 'internal_link_count', 0) for article in self.articles]
        
        # å„ç‰¹å¾´é‡ã«ã¤ã„ã¦åˆ†å¸ƒåˆ†æ
        char_count_dist_stats = self._analyze_distribution(char_counts, "char_count")
        image_count_dist_stats = self._analyze_distribution(image_counts, "image_count")
        heading_count_dist_stats = self._analyze_distribution(heading_counts, "heading_count")
        
        # â˜… æ–°ã—ã„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆåˆ†æ
        video_count_dist_stats = self._analyze_distribution(video_counts, "video_count")
        table_count_dist_stats = self._analyze_distribution(table_counts, "table_count")
        list_item_count_dist_stats = self._analyze_distribution(list_item_counts, "list_item_count")
        external_link_count_dist_stats = self._analyze_distribution(external_link_counts, "external_link_count")
        internal_link_count_dist_stats = self._analyze_distribution(internal_link_counts, "internal_link_count")
        
        section_char_counts = []
        for article in self.articles:
            if hasattr(article, 'headings') and article.headings is not None:
                for heading_node in self._extract_headings_flat(article.headings):
                    if 'char_count_section' in heading_node and isinstance(heading_node['char_count_section'], (int, float)):
                        section_char_counts.append(heading_node['char_count_section'])
        
        section_char_dist_stats = self._analyze_distribution(section_char_counts, "section_char_count")
        
        result = {
            "åˆ†æå¯¾è±¡è¨˜äº‹æ•°": len(self.articles),
            "article_count": len(self.articles),
            "æ–‡å­—æ•°åˆ†æ": {
                "èª¬æ˜": "è¨˜äº‹å…¨ä½“ã®æ–‡å­—æ•°åˆ†æ",
                "description_jp": "è¨˜äº‹å…¨ä½“ã®æ–‡å­—æ•°åˆ†æ",
                "çµ±è¨ˆå€¤": char_count_dist_stats,
                "stats": char_count_dist_stats
            },
            "char_count_analysis": {
                "stats": char_count_dist_stats,
                "description_jp": "è¨˜äº‹å…¨ä½“ã®æ–‡å­—æ•°åˆ†æ"
            },
            "ç”»åƒæ•°åˆ†æ": {
                "èª¬æ˜": "è¨˜äº‹å†…ã®ç”»åƒæ•°åˆ†æ",
                "description_jp": "è¨˜äº‹å†…ã®ç”»åƒæ•°åˆ†æ", 
                "çµ±è¨ˆå€¤": image_count_dist_stats,
                "stats": image_count_dist_stats
            },
            "image_count_analysis": {
                "stats": image_count_dist_stats,
                "description_jp": "è¨˜äº‹å†…ã®ç”»åƒæ•°åˆ†æ"
            },
            "è¦‹å‡ºã—æ•°åˆ†æ": {
                "èª¬æ˜": "è¨˜äº‹å†…ã®è¦‹å‡ºã—ç·æ•°åˆ†æ",
                "description_jp": "è¨˜äº‹å†…ã®è¦‹å‡ºã—ç·æ•°åˆ†æ",
                "çµ±è¨ˆå€¤": heading_count_dist_stats,
                "stats": heading_count_dist_stats
            },
            "heading_count_analysis": {
                "stats": heading_count_dist_stats,
                "description_jp": "è¨˜äº‹å†…ã®è¦‹å‡ºã—ç·æ•°åˆ†æ"
            },
            # â˜… æ–°ã—ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆåˆ†æçµæœã‚’è¿½åŠ 
            "å‹•ç”»æ•°åˆ†æ": {
                "èª¬æ˜": "è¨˜äº‹å†…ã®å‹•ç”»ãƒ»iframeåŸ‹ã‚è¾¼ã¿æ•°åˆ†æ",
                "description_jp": "è¨˜äº‹å†…ã®å‹•ç”»ãƒ»iframeåŸ‹ã‚è¾¼ã¿æ•°åˆ†æ",
                "çµ±è¨ˆå€¤": video_count_dist_stats,
                "stats": video_count_dist_stats
            },
            "video_count_analysis": {
                "stats": video_count_dist_stats,
                "description_jp": "è¨˜äº‹å†…ã®å‹•ç”»ãƒ»iframeåŸ‹ã‚è¾¼ã¿æ•°åˆ†æ"
            },
            "ãƒ†ãƒ¼ãƒ–ãƒ«æ•°åˆ†æ": {
                "èª¬æ˜": "è¨˜äº‹å†…ã®ãƒ†ãƒ¼ãƒ–ãƒ«æ•°åˆ†æï¼ˆå¼·èª¿ã‚¹ãƒ‹ãƒšãƒƒãƒˆå¯¾ç­–æŒ‡æ¨™ï¼‰",
                "description_jp": "è¨˜äº‹å†…ã®ãƒ†ãƒ¼ãƒ–ãƒ«æ•°åˆ†æï¼ˆå¼·èª¿ã‚¹ãƒ‹ãƒšãƒƒãƒˆå¯¾ç­–æŒ‡æ¨™ï¼‰",
                "çµ±è¨ˆå€¤": table_count_dist_stats,
                "stats": table_count_dist_stats
            },
            "table_count_analysis": {
                "stats": table_count_dist_stats,
                "description_jp": "è¨˜äº‹å†…ã®ãƒ†ãƒ¼ãƒ–ãƒ«æ•°åˆ†æï¼ˆå¼·èª¿ã‚¹ãƒ‹ãƒšãƒƒãƒˆå¯¾ç­–æŒ‡æ¨™ï¼‰"
            },
            "ãƒªã‚¹ãƒˆé …ç›®æ•°åˆ†æ": {
                "èª¬æ˜": "è¨˜äº‹å†…ã®ãƒªã‚¹ãƒˆé …ç›®ç·æ•°åˆ†æï¼ˆç¶²ç¾…æ€§æŒ‡æ¨™ï¼‰",
                "description_jp": "è¨˜äº‹å†…ã®ãƒªã‚¹ãƒˆé …ç›®ç·æ•°åˆ†æï¼ˆç¶²ç¾…æ€§æŒ‡æ¨™ï¼‰",
                "çµ±è¨ˆå€¤": list_item_count_dist_stats,
                "stats": list_item_count_dist_stats
            },
            "list_item_count_analysis": {
                "stats": list_item_count_dist_stats,
                "description_jp": "è¨˜äº‹å†…ã®ãƒªã‚¹ãƒˆé …ç›®ç·æ•°åˆ†æï¼ˆç¶²ç¾…æ€§æŒ‡æ¨™ï¼‰"
            },
            "å¤–éƒ¨ãƒªãƒ³ã‚¯æ•°åˆ†æ": {
                "èª¬æ˜": "è¨˜äº‹å†…ã®å¤–éƒ¨ãƒªãƒ³ã‚¯æ•°åˆ†æï¼ˆä¿¡é ¼æ€§ãƒ»æ¨©å¨æ€§æŒ‡æ¨™ï¼‰",
                "description_jp": "è¨˜äº‹å†…ã®å¤–éƒ¨ãƒªãƒ³ã‚¯æ•°åˆ†æï¼ˆä¿¡é ¼æ€§ãƒ»æ¨©å¨æ€§æŒ‡æ¨™ï¼‰",
                "çµ±è¨ˆå€¤": external_link_count_dist_stats,
                "stats": external_link_count_dist_stats
            },
            "external_link_count_analysis": {
                "stats": external_link_count_dist_stats,
                "description_jp": "è¨˜äº‹å†…ã®å¤–éƒ¨ãƒªãƒ³ã‚¯æ•°åˆ†æï¼ˆä¿¡é ¼æ€§ãƒ»æ¨©å¨æ€§æŒ‡æ¨™ï¼‰"
            },
            "å†…éƒ¨ãƒªãƒ³ã‚¯æ•°åˆ†æ": {
                "èª¬æ˜": "è¨˜äº‹å†…ã®å†…éƒ¨ãƒªãƒ³ã‚¯æ•°åˆ†æï¼ˆã‚µã‚¤ãƒˆå›éŠæ€§æŒ‡æ¨™ï¼‰",
                "description_jp": "è¨˜äº‹å†…ã®å†…éƒ¨ãƒªãƒ³ã‚¯æ•°åˆ†æï¼ˆã‚µã‚¤ãƒˆå›éŠæ€§æŒ‡æ¨™ï¼‰",
                "çµ±è¨ˆå€¤": internal_link_count_dist_stats,
                "stats": internal_link_count_dist_stats
            },
            "internal_link_count_analysis": {
                "stats": internal_link_count_dist_stats,
                "description_jp": "è¨˜äº‹å†…ã®å†…éƒ¨ãƒªãƒ³ã‚¯æ•°åˆ†æï¼ˆã‚µã‚¤ãƒˆå›éŠæ€§æŒ‡æ¨™ï¼‰"
            },
            "ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ–‡å­—æ•°åˆ†æ": {
                "èª¬æ˜": "å„è¦‹å‡ºã—ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æ–‡å­—æ•°åˆ†æ",
                "description_jp": "å„è¦‹å‡ºã—ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æ–‡å­—æ•°åˆ†æ",
                "çµ±è¨ˆå€¤": section_char_dist_stats,
                "stats": section_char_dist_stats
            },
            "section_char_count_analysis": {
                "stats": section_char_dist_stats,
                "description_jp": "å„è¦‹å‡ºã—ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æ–‡å­—æ•°åˆ†æ"
            },
            "åˆ†æã‚µãƒãƒªãƒ¼": f"æ‹¡å¼µåˆ†æå®Œäº†: {len(self.articles)}è¨˜äº‹ã‚’å¯¾è±¡ã«ã€æ–‡å­—æ•°ãƒ»ç”»åƒæ•°ãƒ»è¦‹å‡ºã—æ§‹é€ ãƒ»å‹•ç”»ãƒ»ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ»ãƒªã‚¹ãƒˆãƒ»ãƒªãƒ³ã‚¯æ§‹é€ ã®çµ±è¨ˆåˆ†æã‚’å®Ÿæ–½ã—ã¾ã—ãŸã€‚",
            "summary_jp": f"æ‹¡å¼µåˆ†æå®Œäº†: {len(self.articles)}è¨˜äº‹ã‚’å¯¾è±¡ã«ã€æ–‡å­—æ•°ãƒ»ç”»åƒæ•°ãƒ»è¦‹å‡ºã—æ§‹é€ ãƒ»å‹•ç”»ãƒ»ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ»ãƒªã‚¹ãƒˆãƒ»ãƒªãƒ³ã‚¯æ§‹é€ ã®çµ±è¨ˆåˆ†æã‚’å®Ÿæ–½ã—ã¾ã—ãŸã€‚"
        }
        
        self.analysis_results["basic_statistics"] = result
        print("åŸºæœ¬çµ±è¨ˆé‡ã®åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        return result

    def _extract_headings_flat(self, headings: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¦‹å‡ºã—éšå±¤ã‚’ãƒ•ãƒ©ãƒƒãƒˆãªãƒªã‚¹ãƒˆã«å¤‰æ›"""
        flat_headings = []
        
        def _flatten_recursive(heading_list):
            for heading in heading_list:
                if not isinstance(heading, dict): # ã‚¹ã‚­ãƒƒãƒ—ã¾ãŸã¯ãƒ­ã‚°è¨˜éŒ²
                    # print(f"Warning: Expected dict, got {type(heading)}. Skipping item: {heading}")
                    continue
                flat_headings.append({
                    'level': heading.get('level'),
                    'text': heading.get('text', ''),
                    'semantic_type': heading.get('semantic_type', 'body'),
                    'char_count_section': heading.get('char_count_section', 0)
                })
                if heading.get('children'):
                    _flatten_recursive(heading['children'])
        
        if headings: 
             _flatten_recursive(headings)
        return flat_headings

    async def _analyze_frequent_headings(self) -> Dict[str, Any]:
        """ç«¶åˆè¨˜äº‹é–“ã§ã®é »å‡ºè¦‹å‡ºã—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æã™ã‚‹ï¼ˆGemini AI enhancedç‰ˆï¼‰"""
        print("é »å‡ºè¦‹å‡ºã—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æã‚’é–‹å§‹ã—ã¾ã™ï¼ˆGemini AI enhancedï¼‰...")
        
        if not self.articles:
            return {
                "ã‚¨ãƒ©ãƒ¼": "åˆ†æå¯¾è±¡ã®è¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“ã€‚",
                "error": "No articles available for analysis."
            }

        # ã™ã¹ã¦ã®è¦‹å‡ºã—ã‚’åé›†
        all_headings_with_context = []
        for i, article in enumerate(self.articles):
            if not hasattr(article, 'headings') or not article.headings:
                continue
            
            flat_headings = self._extract_headings_flat(article.headings)
            for heading in flat_headings:
                all_headings_with_context.append({
                    'text': heading.get('text', ''),
                    'level': heading.get('level'),
                    'semantic_type': heading.get('semantic_type', 'body'),
                    'article_index': i,
                    'article_url': getattr(article, 'url', f'è¨˜äº‹{i+1}'),
                    'char_count_section': heading.get('char_count_section', 0)
                })

        # 1. å®Œå…¨ä¸€è‡´ã®é »å‡ºè¦‹å‡ºã—åˆ†æ
        exact_matches = Counter()
        for heading in all_headings_with_context:
            text = heading['text'].strip()
            if text and len(text) > 0:
                exact_matches[text] += 1

        frequent_exact = [(text, count) for text, count in exact_matches.items() if count >= 2]
        frequent_exact.sort(key=lambda x: x[1], reverse=True)

        # 2. ãƒ¬ãƒ™ãƒ«åˆ¥é »å‡ºè¦‹å‡ºã—åˆ†æ
        level_based_analysis = {}
        for level in range(1, 7):
            level_headings = [h['text'].strip() for h in all_headings_with_context 
                             if h['level'] == level and h['text'].strip()]
            if level_headings:
                level_counter = Counter(level_headings)
                frequent_in_level = [(text, count) for text, count in level_counter.items() if count >= 2]
                frequent_in_level.sort(key=lambda x: x[1], reverse=True)
                level_based_analysis[f'h{level}'] = {
                    'ç·æ•°': len(level_headings),
                    'é »å‡ºè¦‹å‡ºã—': frequent_in_level[:10],  # ä¸Šä½10å€‹
                    'ä¸€æ„è¦‹å‡ºã—æ•°': len(set(level_headings))
                }

        # 3. æ„å‘³åˆ†é¡åˆ¥é »å‡ºè¦‹å‡ºã—åˆ†æ
        semantic_based_analysis = {}
        for semantic_type in ['introduction', 'body', 'conclusion', 'faq', 'references']:
            semantic_headings = [h['text'].strip() for h in all_headings_with_context 
                               if h['semantic_type'] == semantic_type and h['text'].strip()]
            if semantic_headings:
                semantic_counter = Counter(semantic_headings)
                frequent_in_semantic = [(text, count) for text, count in semantic_counter.items() if count >= 2]
                frequent_in_semantic.sort(key=lambda x: x[1], reverse=True)
                semantic_based_analysis[semantic_type] = {
                    'ç·æ•°': len(semantic_headings),
                    'é »å‡ºè¦‹å‡ºã—': frequent_in_semantic[:10],
                    'ä¸€æ„è¦‹å‡ºã—æ•°': len(set(semantic_headings))
                }

        # 4. â˜… Gemini APIã‚’ä½¿ã£ãŸé«˜åº¦ãªé »å‡ºå˜èªãƒ»é¡ä¼¼è¦‹å‡ºã—åˆ†æ
        print("   ğŸ¤– Gemini APIã§é »å‡ºå˜èªãƒ»é¡ä¼¼è¦‹å‡ºã—ã‚’åˆ†æä¸­...")
        gemini_analysis = await self._analyze_headings_with_gemini(all_headings_with_context)

        result = {
            "å…¨è¦‹å‡ºã—çµ±è¨ˆ": {
                "ç·è¦‹å‡ºã—æ•°": len(all_headings_with_context),
                "ä¸€æ„è¦‹å‡ºã—æ•°": len(set(h['text'].strip() for h in all_headings_with_context if h['text'].strip())),
                "é‡è¤‡è¦‹å‡ºã—æ•°": len(all_headings_with_context) - len(set(h['text'].strip() for h in all_headings_with_context if h['text'].strip()))
            },
            "å®Œå…¨ä¸€è‡´é »å‡ºè¦‹å‡ºã—": {
                "èª¬æ˜": "è¤‡æ•°è¨˜äº‹ã§å…¨ãåŒã˜ãƒ†ã‚­ã‚¹ãƒˆãŒä½¿ã‚ã‚Œã¦ã„ã‚‹è¦‹å‡ºã—",
                "ãƒˆãƒƒãƒ—20": frequent_exact[:20],
                "é »å‡ºè¦‹å‡ºã—ç·æ•°": len(frequent_exact)
            },
            "ãƒ¬ãƒ™ãƒ«åˆ¥é »å‡ºåˆ†æ": level_based_analysis,
            "æ„å‘³åˆ†é¡åˆ¥é »å‡ºåˆ†æ": semantic_based_analysis,
            "Gemini_AIåˆ†æçµæœ": gemini_analysis
        }

        print(f"âœ… é »å‡ºè¦‹å‡ºã—åˆ†æå®Œäº†: {len(all_headings_with_context)}å€‹ã®è¦‹å‡ºã—ã‚’åˆ†æ")
        print(f"   ğŸ“Š å®Œå…¨ä¸€è‡´é »å‡ºè¦‹å‡ºã—: {len(frequent_exact)}ç¨®é¡")
        if "ã‚¨ãƒ©ãƒ¼" not in gemini_analysis:
            print(f"   ğŸ¤– Gemini AIåˆ†æ: æˆåŠŸ")
        else:
            print(f"   âŒ Gemini AIåˆ†æ: {gemini_analysis.get('ã‚¨ãƒ©ãƒ¼', 'ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ')}")
        
        return result

    async def _analyze_headings_with_gemini(self, headings_with_context: List[Dict]) -> Dict[str, Any]:
        """Gemini APIã‚’ä½¿ç”¨ã—ãŸé »å‡ºå˜èªãƒ»é¡ä¼¼è¦‹å‡ºã—ã®é«˜ç²¾åº¦åˆ†æ"""
        
        try:
            from app.core.config import settings
            import google.generativeai as genai
            
            if not settings.gemini_api_key:
                return {
                    "ã‚¨ãƒ©ãƒ¼": "Gemini APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚",
                    "error": "Gemini API key not configured."
                }
            
            setup_genai_client()
            model = genai.GenerativeModel('gemini-1.5-flash-latest')
            
            # è¦‹å‡ºã—ãƒªã‚¹ãƒˆã®æº–å‚™
            headings_list = [h['text'].strip() for h in headings_with_context if h['text'].strip()]
            
            # è¨˜äº‹åˆ¥è¦‹å‡ºã—æƒ…å ±ã®æº–å‚™
            articles_headings = {}
            for h in headings_with_context:
                article_index = h['article_index']
                if article_index not in articles_headings:
                    articles_headings[article_index] = {
                        'url': h['article_url'],
                        'headings': []
                    }
                articles_headings[article_index]['headings'].append({
                    'text': h['text'],
                    'level': h['level'],
                    'semantic_type': h['semantic_type']
                })

            # Gemini APIã¸ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
            prompt = f"""
ã‚ãªãŸã¯ã€SEOã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ç«¶åˆè¨˜äº‹ã®è¦‹å‡ºã—ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦ã€é »å‡ºå˜èªãƒ‘ã‚¿ãƒ¼ãƒ³ã¨é¡ä¼¼è¦‹å‡ºã—ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ç‰¹å®šã—ã¦ãã ã•ã„ã€‚

ã€åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã€‘
å…¨è¦‹å‡ºã—ãƒªã‚¹ãƒˆ: {json.dumps(headings_list, ensure_ascii=False, indent=2)}

è¨˜äº‹åˆ¥è¦‹å‡ºã—æ§‹é€ : {json.dumps(articles_headings, ensure_ascii=False, indent=2)}
-
ã€åˆ†æè¦æ±‚ã€‘
ä»¥ä¸‹ã®é …ç›®ã«ã¤ã„ã¦è©³ç´°ã«åˆ†æã—ã€JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š

1. é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ: è¦‹å‡ºã—ã«å¤šãä½¿ã‚ã‚Œã¦ã„ã‚‹é‡è¦ãªå˜èªãƒ»ãƒ•ãƒ¬ãƒ¼ã‚º
2. é¡ä¼¼è¦‹å‡ºã—ã‚°ãƒ«ãƒ¼ãƒ—: æ„å‘³çš„ã«é¡ä¼¼ã—ã¦ã„ã‚‹è¦‹å‡ºã—ã®ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
3. è¦‹å‡ºã—ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ: ã‚ˆãä½¿ã‚ã‚Œã‚‹è¦‹å‡ºã—ã®æ§‹é€ ãƒ‘ã‚¿ãƒ¼ãƒ³
4. ãƒˆãƒ”ãƒƒã‚¯åˆ†æ: ä¸»è¦ãªãƒˆãƒ”ãƒƒã‚¯ãƒ»ãƒ†ãƒ¼ãƒã®ç‰¹å®š
5. SEOæˆ¦ç•¥çš„ã‚¤ãƒ³ã‚µã‚¤ãƒˆ: ã“ã‚Œã‚‰ã®è¦‹å‡ºã—åˆ†æã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹SEOæˆ¦ç•¥ã®ææ¡ˆ

ã€å‡ºåŠ›å½¢å¼ã€‘
{{
  "é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ": {{
    "é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": [
      {{
        "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": "string",
        "å‡ºç¾å›æ•°": number,
        "ä½¿ç”¨è¨˜äº‹æ•°": number,
        "é‡è¦åº¦": "é«˜|ä¸­|ä½",
        "SEOä¾¡å€¤": "string"
      }}
    ],
    "é‡è¦ãƒ•ãƒ¬ãƒ¼ã‚º": [
      {{
        "ãƒ•ãƒ¬ãƒ¼ã‚º": "string", 
        "å‡ºç¾å›æ•°": number,
        "æ–‡è„ˆ": "string"
      }}
    ]
  }},
  "é¡ä¼¼è¦‹å‡ºã—ã‚°ãƒ«ãƒ¼ãƒ—": [
    {{
      "ã‚°ãƒ«ãƒ¼ãƒ—å": "string",
      "è¦‹å‡ºã—ãƒªã‚¹ãƒˆ": ["string1", "string2", ...],
      "é¡ä¼¼ç†ç”±": "string",
      "å…±é€šãƒ†ãƒ¼ãƒ": "string",
      "SEOåŠ¹æœ": "string"
    }}
  ],
  "è¦‹å‡ºã—ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ": {{
    "é »å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³": [
      {{
        "ãƒ‘ã‚¿ãƒ¼ãƒ³": "string",
        "ä¾‹": ["string1", "string2"],
        "ä½¿ç”¨é »åº¦": "é«˜|ä¸­|ä½"
      }}
    ],
    "ãƒ¬ãƒ™ãƒ«åˆ¥ç‰¹å¾´": {{
      "h1": "string",
      "h2": "string", 
      "h3": "string"
    }}
  }},
  "ä¸»è¦ãƒˆãƒ”ãƒƒã‚¯åˆ†æ": [
    {{
      "ãƒˆãƒ”ãƒƒã‚¯å": "string",
      "é–¢é€£è¦‹å‡ºã—": ["string1", "string2"],
      "é‡è¦åº¦": number,
      "èª¬æ˜": "string"
    }}
  ],
  "SEOæˆ¦ç•¥çš„ã‚¤ãƒ³ã‚µã‚¤ãƒˆ": {{
    "ç«¶åˆã§é »å‡ºã™ã‚‹å¿…é ˆãƒˆãƒ”ãƒƒã‚¯": ["string1", "string2"],
    "å·®åˆ¥åŒ–ã®ãƒãƒ£ãƒ³ã‚¹": ["string1", "string2"], 
    "æ¨å¥¨è¦‹å‡ºã—æˆ¦ç•¥": "string",
    "é¿ã‘ã‚‹ã¹ããƒ‘ã‚¿ãƒ¼ãƒ³": ["string1", "string2"]
  }},
  "åˆ†æã‚µãƒãƒªãƒ¼": {{
    "ç·è¦‹å‡ºã—æ•°": number,
    "ä¸»è¦ãªç«¶åˆæˆ¦ç•¥": "string",
    "æœ€ã‚‚é‡è¦ãªç™ºè¦‹": "string"
  }}
}}

ã€åˆ†ææ™‚ã®æ³¨æ„ç‚¹ã€‘
- åŒç¾©èªãƒ»é¡ç¾©èªã‚’è€ƒæ…®ã—ãŸåˆ†æã‚’è¡Œã£ã¦ãã ã•ã„
- SEOã®è¦³ç‚¹ã‹ã‚‰ä¾¡å€¤ã®é«˜ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é‡è¦–ã—ã¦ãã ã•ã„  
- ç«¶åˆè¨˜äº‹ã®æˆ¦ç•¥çš„æ„å›³ã‚’èª­ã¿å–ã£ã¦åˆ†æã—ã¦ãã ã•ã„
- æ—¥æœ¬èªã®è¡¨è¨˜æºã‚Œï¼ˆã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãƒ»æ¼¢å­—ï¼‰ã‚‚è€ƒæ…®ã—ã¦ãã ã•ã„
"""

            print("   ğŸ“¤ Gemini APIã«è¦‹å‡ºã—åˆ†æã‚’ä¾é ¼ä¸­...")
            generation_config = genai.types.GenerationConfig(
                response_mime_type="application/json",
                temperature=0.3
            )
            
            response = await model.generate_content_async(
                contents=[prompt],
                generation_config=generation_config
            )
            
            if not response.text:
                return {
                    "ã‚¨ãƒ©ãƒ¼": "Gemini APIã‹ã‚‰ã®å¿œç­”ãŒç©ºã§ã—ãŸã€‚",
                    "error": "Empty response from Gemini API."
                }

            try:
                gemini_result = json.loads(response.text)
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                enhanced_result = {
                    "åˆ†æå®Ÿè¡Œæ—¥æ™‚": datetime.datetime.now().isoformat(),
                    "åˆ†æå¯¾è±¡è¦‹å‡ºã—æ•°": len(headings_list),
                    "åˆ†æå¯¾è±¡è¨˜äº‹æ•°": len(articles_headings),
                    "åˆ†ææ‰‹æ³•": "Gemini AI ã«ã‚ˆã‚‹æ„å‘³çš„åˆ†æ",
                    **gemini_result
                }
                
                print("   âœ… Gemini APIã«ã‚ˆã‚‹è¦‹å‡ºã—åˆ†æå®Œäº†")
                return enhanced_result
                
            except json.JSONDecodeError as e:
                return {
                    "ã‚¨ãƒ©ãƒ¼": f"Gemini APIã®å¿œç­”ã‚’JSONã¨ã—ã¦è§£æã§ãã¾ã›ã‚“ã§ã—ãŸ: {str(e)}",
                    "error": f"Failed to parse Gemini API response as JSON: {str(e)}",
                    "ç”Ÿã®å¿œç­”": response.text[:500]
                }

        except Exception as e:
            return {
                "ã‚¨ãƒ©ãƒ¼": f"Gemini APIå‘¼ã³å‡ºã—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
                "error": f"Error during Gemini API call: {str(e)}"
            }

    def analyze_heading_structure(self) -> Dict[str, Any]:
        """è¨˜äº‹ç¾¤ã®è¦‹å‡ºã—æ§‹é€ ã‚’åˆ†æã™ã‚‹"""
        print("è¦‹å‡ºã—æ§‹é€ ã®åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
        if not self.articles:
            print("åˆ†æå¯¾è±¡ã®è¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“ã€‚è¦‹å‡ºã—æ§‹é€ åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            self.analysis_results["heading_structure"] = {"message": "è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚"}
            return self.analysis_results["heading_structure"]

        all_flat_headings = []
        level_usage_per_article_list = []
        max_depth_per_article_list = []
        heading_text_lengths_all = []
        heading_text_lengths_by_level = {f'h{i}': [] for i in range(1, 7)}

        for article in self.articles:
            if not hasattr(article, 'headings') or article.headings is None:
                level_usage_per_article_list.append({f'h{i}': 0 for i in range(1, 7)})
                max_depth_per_article_list.append(0)
                continue

            flat_headings = self._extract_headings_flat(article.headings)
            all_flat_headings.extend(flat_headings)
            
            current_article_level_usage = Counter()
            current_max_depth = 0
            for heading in flat_headings:
                level = heading.get('level')
                if level and isinstance(level, int) and 1 <= level <= 6:
                    current_article_level_usage[f'h{level}'] += 1
                    if level > current_max_depth:
                        current_max_depth = level
                    
                    text = heading.get('text', '')
                    if isinstance(text, str):
                        heading_text_lengths_all.append(len(text))
                        heading_text_lengths_by_level[f'h{level}'].append(len(text))
            
            level_usage_per_article_list.append(dict(current_article_level_usage))
            max_depth_per_article_list.append(current_max_depth)

        total_level_distribution = Counter()
        for usage in level_usage_per_article_list:
            total_level_distribution.update(usage)
        
        avg_level_usage = {level: total_level_distribution.get(level, 0) / len(self.articles) 
                           for level in [f'h{i}' for i in range(1,7)]}
        
        total_headings_count = sum(total_level_distribution.values())
        percentage_level_usage = {level: total_level_distribution.get(level, 0) / total_headings_count if total_headings_count else 0
                                  for level in [f'h{i}' for i in range(1,7)]}
        
        # ãƒ†ã‚­ã‚¹ãƒˆé•·ã®çµ±è¨ˆ
        text_length_stats_all = self._analyze_distribution(heading_text_lengths_all, "all_headings_text_length")
        text_length_stats_by_level = {}
        for level_key, lengths in heading_text_lengths_by_level.items():
            if lengths: # ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®ã¿åˆ†æ
                 text_length_stats_by_level[level_key] = self._analyze_distribution(lengths, f"{level_key}_text_length")
            else:
                 text_length_stats_by_level[level_key] = {"message": f"{level_key}ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"}                             

        result = {
            "è¨˜äº‹åˆ¥è¦‹å‡ºã—ãƒ¬ãƒ™ãƒ«ä½¿ç”¨çŠ¶æ³": level_usage_per_article_list,
            "level_usage_per_article": level_usage_per_article_list,
            "å…¨è¨˜äº‹ã§ã®è¦‹å‡ºã—ãƒ¬ãƒ™ãƒ«åˆ¥ç·æ•°": dict(total_level_distribution),
            "total_level_distribution": dict(total_level_distribution),
            "è¦‹å‡ºã—ãƒ¬ãƒ™ãƒ«åˆ¥å¹³å‡ä½¿ç”¨æ•°ï¼ˆè¨˜äº‹ã‚ãŸã‚Šï¼‰": avg_level_usage,
            "average_level_usage": avg_level_usage,
            "è¦‹å‡ºã—ãƒ¬ãƒ™ãƒ«åˆ¥ä½¿ç”¨å‰²åˆï¼ˆå…¨è¦‹å‡ºã—ä¸­ï¼‰": percentage_level_usage,
            "percentage_level_usage": percentage_level_usage,
            "è¨˜äº‹åˆ¥æœ€å¤§è¦‹å‡ºã—æ·±åº¦": max_depth_per_article_list,
            "max_depth_per_article": max_depth_per_article_list,
            "å¹³å‡æœ€å¤§è¦‹å‡ºã—æ·±åº¦": np.mean(max_depth_per_article_list) if max_depth_per_article_list else 0,
            "average_max_depth": np.mean(max_depth_per_article_list) if max_depth_per_article_list else 0,
            "æœ€ã‚‚å¤šã„æœ€å¤§è¦‹å‡ºã—æ·±åº¦": Counter(max_depth_per_article_list).most_common(1)[0][0] if max_depth_per_article_list else 0,
            "most_common_max_depth": Counter(max_depth_per_article_list).most_common(1)[0][0] if max_depth_per_article_list else 0,
            "æœ€å¤§è¦‹å‡ºã—æ·±åº¦ã®åˆ†æ•£": np.var(max_depth_per_article_list) if max_depth_per_article_list else 0,
            "depth_variance": np.var(max_depth_per_article_list) if max_depth_per_article_list else 0,
            "è¦‹å‡ºã—ãƒ†ã‚­ã‚¹ãƒˆé•·åˆ†æ": {
                "å…¨è¦‹å‡ºã—ãƒ†ã‚­ã‚¹ãƒˆé•·ã®çµ±è¨ˆ": text_length_stats_all,
                "overall": text_length_stats_all,
                "è¦‹å‡ºã—ãƒ¬ãƒ™ãƒ«åˆ¥ãƒ†ã‚­ã‚¹ãƒˆé•·ã®çµ±è¨ˆ": text_length_stats_by_level,
                "by_level": text_length_stats_by_level
            },
            "heading_text_length_analysis": {
                "overall": text_length_stats_all,
                "overall_jp": "å…¨è¦‹å‡ºã—ãƒ†ã‚­ã‚¹ãƒˆé•·ã®çµ±è¨ˆ",
                "by_level": text_length_stats_by_level,
                "by_level_jp": "è¦‹å‡ºã—ãƒ¬ãƒ™ãƒ«åˆ¥ãƒ†ã‚­ã‚¹ãƒˆé•·ã®çµ±è¨ˆ"
            },
            "åˆ†æã‚µãƒãƒªãƒ¼": f"è¦‹å‡ºã—æ§‹é€ åˆ†æå®Œäº†: å…¨{len(self.articles)}è¨˜äº‹ã‹ã‚‰{total_headings_count}å€‹ã®è¦‹å‡ºã—ã‚’åˆ†æã€‚å¹³å‡æœ€å¤§æ·±åº¦ã¯H{np.mean(max_depth_per_article_list) if max_depth_per_article_list else 0:.1f}ãƒ¬ãƒ™ãƒ«ã§ã™ã€‚",
            "summary_jp": f"è¦‹å‡ºã—æ§‹é€ åˆ†æå®Œäº†: å…¨{len(self.articles)}è¨˜äº‹ã‹ã‚‰{total_headings_count}å€‹ã®è¦‹å‡ºã—ã‚’åˆ†æã€‚å¹³å‡æœ€å¤§æ·±åº¦ã¯H{np.mean(max_depth_per_article_list) if max_depth_per_article_list else 0:.1f}ãƒ¬ãƒ™ãƒ«ã§ã™ã€‚"
        }

        self.analysis_results["heading_structure"] = result
        print("è¦‹å‡ºã—æ§‹é€ ã®åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        return result

    def analyze_content_patterns(self) -> Dict[str, Any]:
        """
        è¤‡æ•°ã®ç«¶åˆè¨˜äº‹ã‹ã‚‰å…±é€šã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æã—ã¾ã™ã€‚
        ä¾‹ãˆã°ã€é »å‡ºã™ã‚‹è¦‹å‡ºã—ã®ãƒˆãƒ”ãƒƒã‚¯ã€æ§‹é€ ã®å…±é€šæ€§ãªã©ã‚’æŠ½å‡ºã—ã¾ã™ã€‚

        Returns:
            åˆ†æã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æƒ…å ±ã‚’å«ã‚€è¾æ›¸ã€‚
        """
        print(f"ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æã‚’é–‹å§‹ã—ã¾ã™ã€‚å¯¾è±¡è¨˜äº‹æ•°: {len(self.articles)}")
        
        # TODO: ã“ã“ã«çµ±è¨ˆãƒ™ãƒ¼ã‚¹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã‚’å®Ÿè£…ã—ã¦ã„ã
        # ä¾‹: å…±é€šè¦‹å‡ºã—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®é »åº¦ã€æ„å‘³çš„åˆ†é¡ã®å‡ºç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ãªã©

        # ç¾çŠ¶ã¯åŸºæœ¬çµ±è¨ˆã¨è¦‹å‡ºã—æ§‹é€ ã®å‘¼ã³å‡ºã—ã«ä¾å­˜ã™ã‚‹å½¢ã«ã™ã‚‹ã‹ã€
        # ã¾ãŸã¯ã€ã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ç‹¬è‡ªã®åˆ†æé …ç›®ã‚’å®šç¾©ã™ã‚‹ã€‚
        # ã“ã“ã§ã¯ä¸€æ—¦ã€analyze_heading_structureã®çµæœã®ä¸€éƒ¨ã‚’å‚ç…§ã™ã‚‹ãƒ€ãƒŸãƒ¼å®Ÿè£…ã¨ã™ã‚‹ã€‚
        if "heading_structure" not in self.analysis_results:
            self.analyze_heading_structure() # äº‹å‰ã«å®Ÿè¡Œã—ã¦ãŠã

        patterns_result = {
            "å®Ÿè£…çŠ¶æ³": "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã¯éƒ¨åˆ†çš„ã«å®Ÿè£…ä¸­ã§ã™ã€‚",
            "message": "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã¯éƒ¨åˆ†çš„ã«å®Ÿè£…ä¸­ã§ã™ã€‚",
            "åˆ†æå¯¾è±¡è¨˜äº‹æ•°": len(self.articles),
            "analyzed_article_count": len(self.articles),
            "å…±é€šè¦‹å‡ºã—ãƒ¬ãƒ™ãƒ«ä½¿ç”¨çŠ¶æ³": self.analysis_results.get("heading_structure", {}).get("total_level_distribution"),
            "common_heading_levels_summary": self.analysis_results.get("heading_structure", {}).get("total_level_distribution"),
            "åˆ†æã‚µãƒãƒªãƒ¼": f"ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ: {len(self.articles)}è¨˜äº‹ã‚’åˆ†æä¸­ï¼ˆæ©Ÿèƒ½ã¯ä»Šå¾Œæ‹¡å¼µäºˆå®šï¼‰",
            "summary_jp": f"ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ: {len(self.articles)}è¨˜äº‹ã‚’åˆ†æä¸­ï¼ˆæ©Ÿèƒ½ã¯ä»Šå¾Œæ‹¡å¼µäºˆå®šï¼‰"
        }
        
        self.analysis_results["content_patterns"] = patterns_result
        print("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼ˆéƒ¨åˆ†å®Ÿè£…ï¼‰ã€‚")
        return patterns_result



    def _analyze_frequent_headings_sync(self) -> Dict[str, Any]:
        """ç«¶åˆè¨˜äº‹é–“ã§ã®é »å‡ºè¦‹å‡ºã—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æã™ã‚‹ï¼ˆåŒæœŸç‰ˆãƒ»åŸºæœ¬åˆ†æï¼‰"""
        print("é »å‡ºè¦‹å‡ºã—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æã‚’é–‹å§‹ã—ã¾ã™ï¼ˆåŸºæœ¬ç‰ˆï¼‰...")
        
        if not self.articles:
            return {
                "ã‚¨ãƒ©ãƒ¼": "åˆ†æå¯¾è±¡ã®è¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“ã€‚",
                "error": "No articles available for analysis."
            }

        # ã™ã¹ã¦ã®è¦‹å‡ºã—ã‚’åé›†
        all_headings_with_context = []
        for i, article in enumerate(self.articles):
            if not hasattr(article, 'headings') or not article.headings:
                continue
            
            flat_headings = self._extract_headings_flat(article.headings)
            for heading in flat_headings:
                all_headings_with_context.append({
                    'text': heading.get('text', ''),
                    'level': heading.get('level'),
                    'semantic_type': heading.get('semantic_type', 'body'),
                    'article_index': i,
                    'article_url': getattr(article, 'url', f'è¨˜äº‹{i+1}'),
                    'char_count_section': heading.get('char_count_section', 0)
                })

        # 1. å®Œå…¨ä¸€è‡´ã®é »å‡ºè¦‹å‡ºã—åˆ†æ
        exact_matches = Counter()
        for heading in all_headings_with_context:
            text = heading['text'].strip()
            if text and len(text) > 0:
                exact_matches[text] += 1

        frequent_exact = [(text, count) for text, count in exact_matches.items() if count >= 2]
        frequent_exact.sort(key=lambda x: x[1], reverse=True)

        # 2. ãƒ¬ãƒ™ãƒ«åˆ¥é »å‡ºè¦‹å‡ºã—åˆ†æ
        level_based_analysis = {}
        for level in range(1, 7):
            level_headings = [h['text'].strip() for h in all_headings_with_context 
                             if h['level'] == level and h['text'].strip()]
            if level_headings:
                level_counter = Counter(level_headings)
                frequent_in_level = [(text, count) for text, count in level_counter.items() if count >= 2]
                frequent_in_level.sort(key=lambda x: x[1], reverse=True)
                level_based_analysis[f'h{level}'] = {
                    'ç·æ•°': len(level_headings),
                    'é »å‡ºè¦‹å‡ºã—': frequent_in_level[:10],  # ä¸Šä½10å€‹
                    'ä¸€æ„è¦‹å‡ºã—æ•°': len(set(level_headings))
                }

        # 3. æ„å‘³åˆ†é¡åˆ¥é »å‡ºè¦‹å‡ºã—åˆ†æ
        semantic_based_analysis = {}
        for semantic_type in ['introduction', 'body', 'conclusion', 'faq', 'references']:
            semantic_headings = [h['text'].strip() for h in all_headings_with_context 
                               if h['semantic_type'] == semantic_type and h['text'].strip()]
            if semantic_headings:
                semantic_counter = Counter(semantic_headings)
                frequent_in_semantic = [(text, count) for text, count in semantic_counter.items() if count >= 2]
                frequent_in_semantic.sort(key=lambda x: x[1], reverse=True)
                semantic_based_analysis[semantic_type] = {
                    'ç·æ•°': len(semantic_headings),
                    'é »å‡ºè¦‹å‡ºã—': frequent_in_semantic[:10],
                    'ä¸€æ„è¦‹å‡ºã—æ•°': len(set(semantic_headings))
                }

        # 4. é¡ä¼¼è¦‹å‡ºã—ã‚°ãƒ«ãƒ¼ãƒ—ã®åŸºæœ¬åˆ†æï¼ˆåŒæœŸç‰ˆãƒ»ç°¡æ˜“ï¼‰
        similarity_groups = {}
        processed_headings = set()
        similarity_group_list = []
        
        for heading in all_headings_with_context:
            text = heading['text'].strip().lower()
            if text in processed_headings or len(text) < 3:
                continue
                
            # ç°¡æ˜“é¡ä¼¼åˆ¤å®šï¼ˆå˜èªã®é‡è¤‡ã«ã‚ˆã‚‹ï¼‰
            similar_headings = []
            text_words = set(text.split())
            
            for other_heading in all_headings_with_context:
                other_text = other_heading['text'].strip().lower()
                if other_text != text and other_text not in processed_headings:
                    other_words = set(other_text.split())
                    # 50%ä»¥ä¸Šã®å˜èªãŒå…±é€šã—ã¦ã„ã‚Œã°é¡ä¼¼ã¨ã¿ãªã™
                    common_words = text_words & other_words
                    if len(common_words) > 0 and len(common_words) / max(len(text_words), len(other_words)) >= 0.5:
                        similar_headings.append(other_heading['text'])
                        processed_headings.add(other_text)
            
            if similar_headings:
                similarity_group_list.append({
                    'ãƒ™ãƒ¼ã‚¹è¦‹å‡ºã—': heading['text'],
                    'é¡ä¼¼è¦‹å‡ºã—': similar_headings,
                    'é¡ä¼¼ã‚°ãƒ«ãƒ¼ãƒ—ã‚µã‚¤ã‚º': len(similar_headings) + 1,
                    'ãƒ™ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«': heading['level']
                })
                processed_headings.add(text)
        
        # é¡ä¼¼ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ã‚µã‚¤ã‚ºé †ã«ã‚½ãƒ¼ãƒˆ
        similarity_group_list.sort(key=lambda x: x['é¡ä¼¼ã‚°ãƒ«ãƒ¼ãƒ—ã‚µã‚¤ã‚º'], reverse=True)
        
        similarity_groups = {
            'é¡ä¼¼ã‚°ãƒ«ãƒ¼ãƒ—ç·æ•°': len(similarity_group_list),
            'ãƒˆãƒƒãƒ—10ã‚°ãƒ«ãƒ¼ãƒ—': similarity_group_list[:10]
        }

        result = {
            "å…¨è¦‹å‡ºã—çµ±è¨ˆ": {
                "ç·è¦‹å‡ºã—æ•°": len(all_headings_with_context),
                "ä¸€æ„è¦‹å‡ºã—æ•°": len(set(h['text'].strip() for h in all_headings_with_context if h['text'].strip())),
                "é‡è¤‡è¦‹å‡ºã—æ•°": len(all_headings_with_context) - len(set(h['text'].strip() for h in all_headings_with_context if h['text'].strip()))
            },
            "å®Œå…¨ä¸€è‡´é »å‡ºè¦‹å‡ºã—": {
                "èª¬æ˜": "è¤‡æ•°è¨˜äº‹ã§å…¨ãåŒã˜ãƒ†ã‚­ã‚¹ãƒˆãŒä½¿ã‚ã‚Œã¦ã„ã‚‹è¦‹å‡ºã—",
                "ãƒˆãƒƒãƒ—20": frequent_exact[:20],
                "é »å‡ºè¦‹å‡ºã—ç·æ•°": len(frequent_exact)
            },
            "ãƒ¬ãƒ™ãƒ«åˆ¥é »å‡ºåˆ†æ": level_based_analysis,
            "æ„å‘³åˆ†é¡åˆ¥é »å‡ºåˆ†æ": semantic_based_analysis,
            "é¡ä¼¼è¦‹å‡ºã—ã‚°ãƒ«ãƒ¼ãƒ—": similarity_groups,
            "åˆ†ææ‰‹æ³•": "åŸºæœ¬çš„ãªçµ±è¨ˆåˆ†æã¨ã‚·ãƒ³ãƒ—ãƒ«ãªé¡ä¼¼åˆ¤å®š"
        }

        print(f"âœ… é »å‡ºè¦‹å‡ºã—åˆ†æå®Œäº†ï¼ˆåŸºæœ¬ç‰ˆï¼‰: {len(all_headings_with_context)}å€‹ã®è¦‹å‡ºã—ã‚’åˆ†æ")
        print(f"   ğŸ“Š å®Œå…¨ä¸€è‡´é »å‡ºè¦‹å‡ºã—: {len(frequent_exact)}ç¨®é¡")
        print(f"   ğŸ”— é¡ä¼¼è¦‹å‡ºã—ã‚°ãƒ«ãƒ¼ãƒ—: {len(similarity_group_list)}ã‚°ãƒ«ãƒ¼ãƒ—")
        
        return result

    def get_full_analysis(self, target_article_headings: Optional[List[Dict[str, Any]]] = None) -> Dict[str, Any]:
        """
        å…¨ã¦ã®åˆ†æã‚’å®Ÿè¡Œã—ã€çµ±åˆã•ã‚ŒãŸçµæœã‚’è¿”ã—ã¾ã™ã€‚
        """
        print("å®Œå…¨ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
        self.analyze_basic_statistics() 
        self.analyze_heading_structure() # â˜… è¦‹å‡ºã—æ§‹é€ åˆ†æã‚’å‘¼ã³å‡ºã—
        
        # â˜… é »å‡ºè¦‹å‡ºã—åˆ†æï¼ˆåŸºæœ¬ç‰ˆï¼‰ã‚’è¿½åŠ 
        frequent_headings_result = self._analyze_frequent_headings_sync()
        self.analysis_results["frequent_headings_basic"] = frequent_headings_result
        
        # â˜… æ–°ã—ã„é«˜åº¦åˆ†æãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ 
        self.analyze_multimedia_strategy() # ãƒãƒ«ãƒãƒ¡ãƒ‡ã‚£ã‚¢æˆ¦ç•¥åˆ†æ
        self.analyze_eeat_factors() # E-E-A-Tè¦å› åˆ†æ
        
        self.analyze_content_patterns()
        
        # åˆ†æçµæœå…¨ä½“ã«æ—¥æœ¬èªã‚µãƒãƒªãƒ¼ã‚’è¿½åŠ 
        self.analysis_results["åˆ†æã‚µãƒãƒªãƒ¼"] = {
            "å®Ÿè¡Œæ—¥æ™‚": datetime.datetime.now().isoformat(),
            "åˆ†æå¯¾è±¡è¨˜äº‹æ•°": len(self.articles),
            "å®Ÿè¡Œã—ãŸåˆ†æé …ç›®": [
                "åŸºæœ¬çµ±è¨ˆåˆ†æï¼ˆæ–‡å­—æ•°ãƒ»ç”»åƒæ•°ãƒ»è¦‹å‡ºã—æ•°ãƒ»å‹•ç”»ãƒ»ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ»ãƒªãƒ³ã‚¯ï¼‰",
                "è¦‹å‡ºã—æ§‹é€ åˆ†æï¼ˆãƒ¬ãƒ™ãƒ«åˆ¥ä½¿ç”¨çŠ¶æ³ãƒ»æ·±åº¦åˆ†æï¼‰", 
                "é »å‡ºè¦‹å‡ºã—åˆ†æï¼ˆGemini AI enhancedï¼‰",
                "ãƒãƒ«ãƒãƒ¡ãƒ‡ã‚£ã‚¢æˆ¦ç•¥åˆ†æï¼ˆå‹•ç”»ãƒ»ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ»ãƒªã‚¹ãƒˆæ´»ç”¨çŠ¶æ³ï¼‰",
                "E-E-A-Tè¦å› åˆ†æï¼ˆå°‚é–€æ€§ãƒ»æ¨©å¨æ€§ãƒ»ä¿¡é ¼æ€§ãƒ»é®®åº¦ï¼‰",
                "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"
            ],
            "å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸": f"{len(self.articles)}è¨˜äº‹ã®å®Œå…¨ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æï¼ˆGemini AI enhancedï¼‰ãŒå®Œäº†ã—ã¾ã—ãŸã€‚",
            "Gemini_AIåˆ†æ": "æœ‰åŠ¹ - æ„å‘³çš„ãªè¦‹å‡ºã—åˆ†æã¨é »å‡ºå˜èªã®é«˜ç²¾åº¦æŠ½å‡ºã‚’å®Ÿè¡Œ"
        }
        self.analysis_results["analysis_summary_jp"] = {
            "å®Ÿè¡Œæ—¥æ™‚": "åˆ†æå®Ÿè¡Œå®Œäº†",
            "åˆ†æå¯¾è±¡è¨˜äº‹æ•°": len(self.articles),
            "å®Ÿè¡Œã—ãŸåˆ†æé …ç›®": [
                "åŸºæœ¬çµ±è¨ˆåˆ†æï¼ˆæ–‡å­—æ•°ãƒ»ç”»åƒæ•°ãƒ»è¦‹å‡ºã—æ•°ï¼‰",
                "è¦‹å‡ºã—æ§‹é€ åˆ†æï¼ˆãƒ¬ãƒ™ãƒ«åˆ¥ä½¿ç”¨çŠ¶æ³ãƒ»æ·±åº¦åˆ†æï¼‰", 
                "é »å‡ºè¦‹å‡ºã—åˆ†æï¼ˆåŸºæœ¬ç‰ˆï¼‰",  # â˜… è¿½åŠ 
                "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æï¼ˆéƒ¨åˆ†å®Ÿè£…ï¼‰",
                "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚®ãƒ£ãƒƒãƒ—æŠ½å‡ºï¼ˆæœªå®Ÿè£…ï¼‰",
                "ç«¶äº‰å„ªä½æ€§ç‰¹å®šï¼ˆæœªå®Ÿè£…ï¼‰"
            ],
            "åˆ©ç”¨å¯èƒ½ãªçµ±è¨ˆå€¤": [
                "å¹³å‡å€¤ãƒ»ä¸­å¤®å€¤ãƒ»æ¨™æº–åå·®",
                "æœ€å¤§å€¤ãƒ»æœ€å°å€¤ãƒ»å››åˆ†ä½æ•°", 
                "ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ãƒ»å¤–ã‚Œå€¤åˆ¤å®šåŸºæº–"
            ],
            "å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸": f"{len(self.articles)}è¨˜äº‹ã®å®Œå…¨ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚",
            "Gemini_AIåˆ†æã«ã¤ã„ã¦": "Gemini AIã‚’ä½¿ã£ãŸé«˜åº¦ãªåˆ†æã¯ get_full_analysis_with_gemini() ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ã”åˆ©ç”¨ãã ã•ã„ã€‚"
        }
        
        print("å®Œå…¨ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        return self.analysis_results

    async def get_full_analysis_with_gemini(self, target_article_headings: Optional[List[Dict[str, Any]]] = None) -> Dict[str, Any]:
        """
        Gemini AIã‚’ä½¿ã£ãŸé«˜åº¦ãªåˆ†æã‚’å«ã‚€å…¨ã¦ã®åˆ†æã‚’å®Ÿè¡Œã—ã€çµ±åˆã•ã‚ŒãŸçµæœã‚’è¿”ã—ã¾ã™ã€‚
        """
        print("Gemini AI enhanced ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
        
        # åŸºæœ¬åˆ†æã‚’å®Ÿè¡Œ
        self.analyze_basic_statistics() 
        self.analyze_heading_structure()
        
        # â˜… Gemini AIã‚’ä½¿ã£ãŸé »å‡ºè¦‹å‡ºã—åˆ†æ
        frequent_headings_gemini_result = await self._analyze_frequent_headings()
        self.analysis_results["frequent_headings_gemini"] = frequent_headings_gemini_result
        
        # â˜… æ–°ã—ã„é«˜åº¦åˆ†æãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ 
        self.analyze_multimedia_strategy() # ãƒãƒ«ãƒãƒ¡ãƒ‡ã‚£ã‚¢æˆ¦ç•¥åˆ†æ
        self.analyze_eeat_factors() # E-E-A-Tè¦å› åˆ†æ
        
        self.analyze_content_patterns()
        
        # åˆ†æçµæœå…¨ä½“ã«æ—¥æœ¬èªã‚µãƒãƒªãƒ¼ã‚’è¿½åŠ 
        self.analysis_results["åˆ†æã‚µãƒãƒªãƒ¼"] = {
            "å®Ÿè¡Œæ—¥æ™‚": "åˆ†æå®Ÿè¡Œå®Œäº†",
            "åˆ†æå¯¾è±¡è¨˜äº‹æ•°": len(self.articles),
            "å®Ÿè¡Œã—ãŸåˆ†æé …ç›®": [
                "åŸºæœ¬çµ±è¨ˆåˆ†æï¼ˆæ–‡å­—æ•°ãƒ»ç”»åƒæ•°ãƒ»è¦‹å‡ºã—æ•°ï¼‰",
                "è¦‹å‡ºã—æ§‹é€ åˆ†æï¼ˆãƒ¬ãƒ™ãƒ«åˆ¥ä½¿ç”¨çŠ¶æ³ãƒ»æ·±åº¦åˆ†æï¼‰", 
                "é »å‡ºè¦‹å‡ºã—åˆ†æï¼ˆGemini AI enhancedï¼‰",  # â˜… è¿½åŠ 
                "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æï¼ˆéƒ¨åˆ†å®Ÿè£…ï¼‰",
                "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚®ãƒ£ãƒƒãƒ—æŠ½å‡ºï¼ˆæœªå®Ÿè£…ï¼‰",
                "ç«¶äº‰å„ªä½æ€§ç‰¹å®šï¼ˆæœªå®Ÿè£…ï¼‰"
            ],
            "åˆ©ç”¨å¯èƒ½ãªçµ±è¨ˆå€¤": [
                "å¹³å‡å€¤ãƒ»ä¸­å¤®å€¤ãƒ»æ¨™æº–åå·®",
                "æœ€å¤§å€¤ãƒ»æœ€å°å€¤ãƒ»å››åˆ†ä½æ•°", 
                "ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ãƒ»å¤–ã‚Œå€¤åˆ¤å®šåŸºæº–"
            ],
            "å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸": f"{len(self.articles)}è¨˜äº‹ã®å®Œå…¨ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æï¼ˆGemini AI enhancedï¼‰ãŒå®Œäº†ã—ã¾ã—ãŸã€‚",
            "Gemini_AIåˆ†æ": "æœ‰åŠ¹ - æ„å‘³çš„ãªè¦‹å‡ºã—åˆ†æã¨é »å‡ºå˜èªã®é«˜ç²¾åº¦æŠ½å‡ºã‚’å®Ÿè¡Œ"
        }
        self.analysis_results["analysis_summary_jp"] = {
            "å®Ÿè¡Œæ—¥æ™‚": "åˆ†æå®Ÿè¡Œå®Œäº†",
            "åˆ†æå¯¾è±¡è¨˜äº‹æ•°": len(self.articles),
            "å®Ÿè¡Œã—ãŸåˆ†æé …ç›®": [
                "åŸºæœ¬çµ±è¨ˆåˆ†æï¼ˆæ–‡å­—æ•°ãƒ»ç”»åƒæ•°ãƒ»è¦‹å‡ºã—æ•°ï¼‰",
                "è¦‹å‡ºã—æ§‹é€ åˆ†æï¼ˆãƒ¬ãƒ™ãƒ«åˆ¥ä½¿ç”¨çŠ¶æ³ãƒ»æ·±åº¦åˆ†æï¼‰", 
                "é »å‡ºè¦‹å‡ºã—åˆ†æï¼ˆGemini AI enhancedï¼‰",  # â˜… è¿½åŠ 
                "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æï¼ˆéƒ¨åˆ†å®Ÿè£…ï¼‰",
                "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚®ãƒ£ãƒƒãƒ—æŠ½å‡ºï¼ˆæœªå®Ÿè£…ï¼‰",
                "ç«¶äº‰å„ªä½æ€§ç‰¹å®šï¼ˆæœªå®Ÿè£…ï¼‰"
            ],
            "åˆ©ç”¨å¯èƒ½ãªçµ±è¨ˆå€¤": [
                "å¹³å‡å€¤ãƒ»ä¸­å¤®å€¤ãƒ»æ¨™æº–åå·®",
                "æœ€å¤§å€¤ãƒ»æœ€å°å€¤ãƒ»å››åˆ†ä½æ•°", 
                "ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ãƒ»å¤–ã‚Œå€¤åˆ¤å®šåŸºæº–"
            ],
            "å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸": f"{len(self.articles)}è¨˜äº‹ã®å®Œå…¨ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æï¼ˆGemini AI enhancedï¼‰ãŒå®Œäº†ã—ã¾ã—ãŸã€‚",
            "Gemini_AIåˆ†æ": "æœ‰åŠ¹ - æ„å‘³çš„ãªè¦‹å‡ºã—åˆ†æã¨é »å‡ºå˜èªã®é«˜ç²¾åº¦æŠ½å‡ºã‚’å®Ÿè¡Œ"
        }
        
        print("Gemini AI enhanced ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        return self.analysis_results

    def _filter_keys_by_language(self, data: Dict[str, Any], language: str = "jp") -> Dict[str, Any]:
        """
        ãƒ‡ãƒ¼ã‚¿å†…ã®ã‚­ãƒ¼ã‚’æŒ‡å®šè¨€èªã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹
        
        Args:
            data: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿
            language: "jp" (æ—¥æœ¬èªã‚­ãƒ¼ã®ã¿) ã¾ãŸã¯ "en" (è‹±èªã‚­ãƒ¼ã®ã¿)
            
        Returns:
            ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
        """
        if not isinstance(data, dict):
            return data
        
        # æ—¥æœ¬èªã‚­ãƒ¼ã¨è‹±èªã‚­ãƒ¼ã®ãƒšã‚¢å®šç¾©
        key_pairs = {
            # åŸºæœ¬çµ±è¨ˆé–¢é€£
            "æ–‡å­—æ•°åˆ†æ": "char_count_analysis",
            "ç”»åƒæ•°åˆ†æ": "image_count_analysis", 
            "è¦‹å‡ºã—æ•°åˆ†æ": "heading_count_analysis",
            "å‹•ç”»æ•°åˆ†æ": "video_count_analysis",
            "ãƒ†ãƒ¼ãƒ–ãƒ«æ•°åˆ†æ": "table_count_analysis",
            "ãƒªã‚¹ãƒˆé …ç›®æ•°åˆ†æ": "list_item_count_analysis",
            "å¤–éƒ¨ãƒªãƒ³ã‚¯æ•°åˆ†æ": "external_link_count_analysis",
            "å†…éƒ¨ãƒªãƒ³ã‚¯æ•°åˆ†æ": "internal_link_count_analysis",
            "ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ–‡å­—æ•°åˆ†æ": "section_char_count_analysis",
            
            # çµ±è¨ˆå€¤é–¢é€£
            "çµ±è¨ˆå€¤": "stats",
            "èª¬æ˜": "description",
            "åˆ†æå¯¾è±¡è¨˜äº‹æ•°": "article_count",
            "åˆ†æã‚µãƒãƒªãƒ¼": "summary_jp",
            
            # è¦‹å‡ºã—æ§‹é€ é–¢é€£
            "è¨˜äº‹åˆ¥è¦‹å‡ºã—ãƒ¬ãƒ™ãƒ«ä½¿ç”¨çŠ¶æ³": "level_usage_per_article",
            "å…¨è¨˜äº‹ã§ã®è¦‹å‡ºã—ãƒ¬ãƒ™ãƒ«åˆ¥ç·æ•°": "total_level_distribution",
            "è¦‹å‡ºã—ãƒ¬ãƒ™ãƒ«åˆ¥å¹³å‡ä½¿ç”¨æ•°ï¼ˆè¨˜äº‹ã‚ãŸã‚Šï¼‰": "average_level_usage",
            "è¦‹å‡ºã—ãƒ¬ãƒ™ãƒ«åˆ¥ä½¿ç”¨å‰²åˆï¼ˆå…¨è¦‹å‡ºã—ä¸­ï¼‰": "percentage_level_usage",
            "è¨˜äº‹åˆ¥æœ€å¤§è¦‹å‡ºã—æ·±åº¦": "max_depth_per_article",
            "å¹³å‡æœ€å¤§è¦‹å‡ºã—æ·±åº¦": "average_max_depth",
            "æœ€ã‚‚å¤šã„æœ€å¤§è¦‹å‡ºã—æ·±åº¦": "most_common_max_depth",
            "æœ€å¤§è¦‹å‡ºã—æ·±åº¦ã®åˆ†æ•£": "depth_variance",
            "è¦‹å‡ºã—ãƒ†ã‚­ã‚¹ãƒˆé•·åˆ†æ": "heading_text_length_analysis",
            
            # çµ±è¨ˆé …ç›®
            "å¹³å‡å€¤": "mean",
            "ä¸­å¤®å€¤": "median", 
            "æ¨™æº–åå·®": "std_dev",
            "åˆ†æ•£": "variance",
            "æœ€å°å€¤": "min",
            "æœ€å¤§å€¤": "max",
            "ç¯„å›²ï¼ˆæœ€å¤§å€¤-æœ€å°å€¤ï¼‰": "range",
            "ç¬¬1å››åˆ†ä½æ•°ï¼ˆ25ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ï¼‰": "q1",
            "ç¬¬3å››åˆ†ä½æ•°ï¼ˆ75ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ï¼‰": "q3",
            "å››åˆ†ä½ç¯„å›²": "iqr",
            "ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«å€¤": "percentiles",
            "å¤–ã‚Œå€¤åˆ¤å®šåŸºæº–": "outlier_thresholds",
            "ãƒ‡ãƒ¼ã‚¿æ•°": "count",
            "çµ±è¨ˆã‚µãƒãƒªãƒ¼": "summary_jp",
            
            # â˜… è¦‹å‡ºã—ææ¡ˆé–¢é€£
            "åˆ†æå®Ÿè¡Œæ—¥æ™‚": "analysis_timestamp",
            "å…¥åŠ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿": "input_parameters",
            "ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": "target_keyword",
            "è¨˜äº‹ç›®çš„": "article_purpose", 
            "ã‚¿ãƒ¼ã‚²ãƒƒãƒˆèª­è€…": "target_audience",
            "ç«¶åˆè¨˜äº‹åˆ†æã‚µãƒãƒªãƒ¼": "competitor_analysis_summary",
            "åˆ†æè¨˜äº‹æ•°": "analyzed_articles",
            "ç·è¦‹å‡ºã—æ•°": "total_headings",
            "é »å‡ºè¦‹å‡ºã—åˆ†æçµæœ": "frequent_headings_analysis",
            "Geminiåˆ†æçµæœ": "gemini_analysis",
            "Gemini_AIåˆ†æçµæœ": "gemini_analysis_results"
        }
        
        filtered_data = {}
        
        if language == "jp":
            # æ—¥æœ¬èªç‰ˆ: æ—¥æœ¬èªã‚­ãƒ¼ã‚’æ®‹ã—ã€å¯¾å¿œã™ã‚‹è‹±èªã‚­ãƒ¼ã¯é™¤å¤–
            jp_keys = set(key_pairs.keys())
            en_keys = set(key_pairs.values())
            
            for key, value in data.items():
                # è‹±èªã‚­ãƒ¼ã§å¯¾å¿œã™ã‚‹æ—¥æœ¬èªã‚­ãƒ¼ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                if key in en_keys:
                    jp_equivalent = None
                    for jp_key, en_key in key_pairs.items():
                        if en_key == key:
                            jp_equivalent = jp_key
                            break
                    if jp_equivalent and jp_equivalent in data:
                        continue
                
                # å†å¸°çš„ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if isinstance(value, dict):
                    filtered_data[key] = self._filter_keys_by_language(value, language)
                elif isinstance(value, list):
                    filtered_data[key] = [
                        self._filter_keys_by_language(item, language) if isinstance(item, dict) else item
                        for item in value
                    ]
                else:
                    filtered_data[key] = value
                    
        elif language == "en":
            # è‹±èªç‰ˆ: è‹±èªã‚­ãƒ¼ã‚’æ®‹ã—ã€æ—¥æœ¬èªã‚­ãƒ¼ã¯é™¤å¤–
            for key, value in data.items():
                # æ—¥æœ¬èªã‚­ãƒ¼ã§å¯¾å¿œã™ã‚‹è‹±èªã‚­ãƒ¼ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã€è‹±èªã‚­ãƒ¼ã®ã¿æ®‹ã™
                if key in key_pairs:
                    en_key = key_pairs[key]
                    if en_key in data:
                        continue  # å¯¾å¿œã™ã‚‹è‹±èªã‚­ãƒ¼ãŒã‚ã‚‹ã®ã§æ—¥æœ¬èªã‚­ãƒ¼ã¯ã‚¹ã‚­ãƒƒãƒ—
                
                # å†å¸°çš„ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if isinstance(value, dict):
                    filtered_data[key] = self._filter_keys_by_language(value, language)
                elif isinstance(value, list):
                    filtered_data[key] = [
                        self._filter_keys_by_language(item, language) if isinstance(item, dict) else item
                        for item in value
                    ]
                else:
                    filtered_data[key] = value
        
        return filtered_data

    def export_to_json(self, filename: str, language: str = "jp"):
        """
        åˆ†æçµæœã‚’JSONå½¢å¼ã§ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›ã—ã¾ã™ã€‚

        Args:
            filename: å‡ºåŠ›ã™ã‚‹JSONãƒ•ã‚¡ã‚¤ãƒ«ã®åå‰ã€‚
            language: "jp" (æ—¥æœ¬èªã‚­ãƒ¼ã®ã¿) ã¾ãŸã¯ "en" (è‹±èªã‚­ãƒ¼ã®ã¿) ã¾ãŸã¯ "both" (ä¸¡æ–¹)
        """
        if language == "both":
            # ä¸¡æ–¹ã®è¨€èªã§å‡ºåŠ›
            base_name = filename.replace('.json', '')
            
            # æ—¥æœ¬èªç‰ˆ
            jp_filename = f"{base_name}_jp.json"
            self.export_to_json(jp_filename, "jp")
            
            # è‹±èªç‰ˆ
            en_filename = f"{base_name}_en.json"
            self.export_to_json(en_filename, "en")
            
            print(f"âœ… ä¸¡è¨€èªç‰ˆJSONåˆ†æãƒ‡ãƒ¼ã‚¿ãŒå‡ºåŠ›ã•ã‚Œã¾ã—ãŸ:")
            print(f"   ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªç‰ˆ: {jp_filename}")
            print(f"   ğŸ‡ºğŸ‡¸ è‹±èªç‰ˆ: {en_filename}")
            return
        
        # è¨€èªåˆ¥ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        filtered_results = self._filter_keys_by_language(self.analysis_results, language)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        if language == "jp":
            export_data = {
                "åˆ†æãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿": {
                    "å®Ÿè¡Œæ—¥æ™‚": datetime.datetime.now().isoformat(),
                    "åˆ†æå¯¾è±¡è¨˜äº‹æ•°": len(self.articles),
                    "ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå½¢å¼": "JSON",
                    "è¨€èª": "æ—¥æœ¬èª"
                },
                "åˆ†æçµæœ": filtered_results
            }
            lang_label = "æ—¥æœ¬èªç‰ˆ"
        else:
            export_data = {
                "analysis_metadata": {
                    "execution_time": datetime.datetime.now().isoformat(),
                    "analyzed_articles": len(self.articles),
                    "export_format": "JSON",
                    "language": "English"
                },
                "analysis_results": filtered_results
            }
            lang_label = "è‹±èªç‰ˆ"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… {lang_label}JSONåˆ†æãƒ‡ãƒ¼ã‚¿ãŒ {filename} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")

    async def _infer_topic_from_articles(self) -> Dict[str, str]:
        """
        è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨é »å‡ºè¦‹å‡ºã—ã‚’åˆ†æã—ã€Gemini ã‚’ä½¿ç”¨ã—ã¦ä¸­å¿ƒçš„ãªãƒˆãƒ”ãƒƒã‚¯ã€
        è¨˜äº‹ã®ç›®çš„ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆèª­è€…ã‚’æ¨æ¸¬ã—ã¾ã™ã€‚
        """
        print("ğŸ¤– Gemini APIã‚’ä½¿ç”¨ã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰ä¸­å¿ƒãƒˆãƒ”ãƒƒã‚¯ã‚’æ¨æ¸¬ä¸­...")
        if not self.articles:
            return {
                "target_keyword": "ä¸æ˜ãªãƒˆãƒ”ãƒƒã‚¯",
                "article_purpose": "ä¸€èˆ¬çš„ãªè¨˜äº‹",
                "target_audience": "ä¸€èˆ¬èª­è€…"
            }

        try:
            from app.core.config import settings
            import google.generativeai as genai

            if not settings.gemini_api_key:
                raise ValueError("Gemini APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            
            setup_genai_client()
            model = genai.GenerativeModel('gemini-2.0-flash')

            # ãƒ‡ãƒ¼ã‚¿åé›†
            article_titles = [getattr(article, 'title', '') for article in self.articles]
            
            if "frequent_headings_basic" not in self.analysis_results:
                self._analyze_frequent_headings_sync()
                
            frequent_headings_result = self.analysis_results.get("frequent_headings_basic", {})
            frequent_headings = [h[0] for h in frequent_headings_result.get("å®Œå…¨ä¸€è‡´é »å‡ºè¦‹å‡ºã—", {}).get("ãƒˆãƒƒãƒ—20", [])[:10]]

            prompt = f"""
ã‚ãªãŸã¯å„ªã‚ŒãŸSEOã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€ã“ã‚Œã‚‰ã®è¨˜äº‹ç¾¤ãŒã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨ã—ã¦ã„ã‚‹ã€Œä¸­å¿ƒçš„ãªæ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€ã€ã€Œè¨˜äº‹ã®ç›®çš„ã€ã€ã€Œã‚¿ãƒ¼ã‚²ãƒƒãƒˆèª­è€…ã€ã‚’æ¨æ¸¬ã—ã¦ãã ã•ã„ã€‚

ã€åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã€‘
â–  è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ« ({len(article_titles)}ä»¶):
{json.dumps(article_titles, ensure_ascii=False, indent=2)}

â–  é »å‡ºã™ã‚‹è¦‹å‡ºã— ({len(frequent_headings)}ä»¶):
{json.dumps(frequent_headings, ensure_ascii=False, indent=2)}

ã€æŒ‡ç¤ºã€‘
ä¸Šè¨˜ã®æƒ…å ±ã‚’åŸºã«ã€æœ€ã‚‚å¯èƒ½æ€§ã®é«˜ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€ç›®çš„ã€èª­è€…å±¤ã‚’ç‰¹å®šã—ã€ä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¤œç´¢çª“ã«å…¥åŠ›ã™ã‚‹ã‚ˆã†ãªå…·ä½“çš„ã§çŸ­ã„ãƒ•ãƒ¬ãƒ¼ã‚ºã«ã—ã¦ãã ã•ã„ã€‚

ã€å‡ºåŠ›å½¢å¼ã€‘
{{
  "target_keyword": "string",
  "article_purpose": "string",
  "target_audience": "string"
}}

ä½™è¨ˆãªèª¬æ˜ã¯å«ã‚ãšã€JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
"""
            generation_config = genai.types.GenerationConfig(
                response_mime_type="application/json",
                temperature=0.1
            )
            
            response = await model.generate_content_async(
                contents=[prompt],
                generation_config=generation_config
            )

            if not response.text:
                raise ValueError("Gemini APIã‹ã‚‰ã®å¿œç­”ãŒç©ºã§ã—ãŸã€‚")

            result = json.loads(response.text)
            print(f"   âœ… ãƒˆãƒ”ãƒƒã‚¯æ¨æ¸¬æˆåŠŸ: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{result.get('target_keyword')}ã€")
            return result

        except Exception as e:
            print(f"   âš ï¸ ãƒˆãƒ”ãƒƒã‚¯æ¨æ¸¬ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¿”ã—ã¾ã™ã€‚")
            return {
                "target_keyword": "åˆ†æã•ã‚ŒãŸãƒˆãƒ”ãƒƒã‚¯",
                "article_purpose": "SEOåŠ¹æœçš„ãªè©³ç´°è¨˜äº‹",
                "target_audience": "ä¸€èˆ¬èª­è€…"
            }

    async def suggest_optimal_headings(
        self, 
        target_keyword: Optional[str] = None, 
        article_purpose: Optional[str] = None,
        target_audience: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ç«¶åˆè¨˜äº‹ã®è¦‹å‡ºã—æ§‹é€ ã‚’ã™ã¹ã¦åˆ†æã—ã€Gemini APIã‚’ä½¿ç”¨ã—ã¦
        SEOåŠ¹æœçš„ãªæœ€é©ãªè¦‹å‡ºã—æ§‹é€ ã‚’ææ¡ˆã—ã¾ã™ã€‚
        ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãªã©ãŒæŒ‡å®šã•ã‚Œãªã„å ´åˆã¯ã€åˆ†ææ¸ˆã¿ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰æ¨æ¸¬ã—ã¾ã™ã€‚

        Args:
            target_keyword: ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€‚ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€‚
            article_purpose: ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€‚è¨˜äº‹ã®ç›®çš„ã€‚
            target_audience: ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€‚ã‚¿ãƒ¼ã‚²ãƒƒãƒˆèª­è€…å±¤ã€‚

        Returns:
            ææ¡ˆã•ã‚ŒãŸæœ€é©ãªè¦‹å‡ºã—æ§‹é€ ã‚’å«ã‚€è¾æ›¸
        """
        print(f"æœ€é©è¦‹å‡ºã—æ§‹é€ ã®ææ¡ˆã‚’é–‹å§‹ã—ã¾ã™...")
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç­‰ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰æ¨æ¸¬ã™ã‚‹
        if not target_keyword or not article_purpose or not target_audience:
            print("...ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰æ¨æ¸¬ã—ã¾ã™ã€‚")
            try:
                inferred_topic = await self._infer_topic_from_articles()
                # æŒ‡å®šã•ã‚Œã¦ã„ãªã„å¼•æ•°ã®ã¿ã€æ¨æ¸¬çµæœã§ä¸Šæ›¸ãã™ã‚‹
                if not target_keyword:
                    target_keyword = inferred_topic.get("target_keyword", "åˆ†æãƒˆãƒ”ãƒƒã‚¯")
                if not article_purpose:
                    article_purpose = inferred_topic.get("article_purpose", "è©³ç´°ã‚¬ã‚¤ãƒ‰")
                if not target_audience:
                    target_audience = inferred_topic.get("target_audience", "ä¸€èˆ¬èª­è€…")
            except Exception as e:
                print(f"âš ï¸ ãƒˆãƒ”ãƒƒã‚¯ã®æ¨æ¸¬ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                target_keyword = target_keyword or "åˆ†æã•ã‚ŒãŸãƒˆãƒ”ãƒƒã‚¯"
                article_purpose = article_purpose or "SEOåŠ¹æœçš„ãªè©³ç´°è¨˜äº‹"
                target_audience = target_audience or "ä¸€èˆ¬èª­è€…"
        
        print(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {target_keyword}")
        print(f"è¨˜äº‹ã®ç›®çš„: {article_purpose}")
        print(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆèª­è€…: {target_audience}")

        if not self.articles:
            return {
                "ã‚¨ãƒ©ãƒ¼": "åˆ†æå¯¾è±¡ã®è¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“ã€‚",
                "error": "No articles available for analysis."
            }

        # Gemini APIã®è¨­å®šç¢ºèª
        try:
            from app.core.config import settings
            import google.generativeai as genai
            
            if not settings.gemini_api_key:
                return {
                    "ã‚¨ãƒ©ãƒ¼": "Gemini APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚",
                    "error": "Gemini API key not configured."
                }
            
            setup_genai_client()
            model = genai.GenerativeModel('gemini-2.0-flash')
            
        except Exception as e:
            return {
                "ã‚¨ãƒ©ãƒ¼": f"Gemini APIè¨­å®šã‚¨ãƒ©ãƒ¼: {str(e)}",
                "error": f"Gemini API configuration error: {str(e)}"
            }

        # ã™ã¹ã¦ã®ç«¶åˆè¨˜äº‹ã‹ã‚‰è¦‹å‡ºã—æ§‹é€ ã‚’æŠ½å‡º
        all_competitor_headings = []
        for i, article in enumerate(self.articles):
            if not hasattr(article, 'headings') or not article.headings:
                continue
            
            flat_headings = self._extract_headings_flat(article.headings)
            competitor_data = {
                "è¨˜äº‹ç•ªå·": i + 1,
                "è¨˜äº‹URL": getattr(article, 'url', f'è¨˜äº‹{i+1}'),
                "è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«": getattr(article, 'title', f'ã‚¿ã‚¤ãƒˆãƒ«{i+1}'),
                "æ–‡å­—æ•°": getattr(article, 'char_count', 0),
                "ç”»åƒæ•°": getattr(article, 'image_count', 0),
                # â˜… æ–°ã—ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæƒ…å ±ã‚’è¿½åŠ 
                "å‹•ç”»æ•°": getattr(article, 'video_count', 0),
                "ãƒ†ãƒ¼ãƒ–ãƒ«æ•°": getattr(article, 'table_count', 0),
                "ãƒªã‚¹ãƒˆé …ç›®æ•°": getattr(article, 'list_item_count', 0),
                "å¤–éƒ¨ãƒªãƒ³ã‚¯æ•°": getattr(article, 'external_link_count', 0),
                "å†…éƒ¨ãƒªãƒ³ã‚¯æ•°": getattr(article, 'internal_link_count', 0),
                # â˜… E-E-A-Té–¢é€£æƒ…å ±ã‚’è¿½åŠ 
                "è‘—è€…æƒ…å ±": getattr(article, 'author_info', None),
                "å…¬é–‹æ—¥": getattr(article, 'publish_date', None),
                "æ›´æ–°æ—¥": getattr(article, 'modified_date', None),
                "æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿": getattr(article, 'schema_types', []),
                "è¦‹å‡ºã—æ§‹é€ ": [
                    {
                        "ãƒ¬ãƒ™ãƒ«": h.get('level'),
                        "ãƒ†ã‚­ã‚¹ãƒˆ": h.get('text'),
                        "æ„å‘³åˆ†é¡": h.get('semantic_type', 'body'),
                        "ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ–‡å­—æ•°": h.get('char_count_section', 0)
                    }
                    for h in flat_headings
                ]
            }
            all_competitor_headings.append(competitor_data)

        # â˜… é »å‡ºè¦‹å‡ºã—åˆ†æã‚’å®Ÿè¡Œ
        print("ğŸ“Š é »å‡ºè¦‹å‡ºã—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æä¸­...")
        frequent_headings_analysis = self._analyze_frequent_headings_sync()

        # â˜… çµ±è¨ˆæƒ…å ±ã®æº–å‚™ï¼ˆæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ï¼‰
        stats_summary = ""
        if "basic_statistics" in self.analysis_results:
            char_stats = self.analysis_results["basic_statistics"]["æ–‡å­—æ•°åˆ†æ"]["çµ±è¨ˆå€¤"]
            image_stats = self.analysis_results["basic_statistics"]["ç”»åƒæ•°åˆ†æ"]["çµ±è¨ˆå€¤"]
            video_stats = self.analysis_results["basic_statistics"]["å‹•ç”»æ•°åˆ†æ"]["çµ±è¨ˆå€¤"]
            table_stats = self.analysis_results["basic_statistics"]["ãƒ†ãƒ¼ãƒ–ãƒ«æ•°åˆ†æ"]["çµ±è¨ˆå€¤"]
            list_stats = self.analysis_results["basic_statistics"]["ãƒªã‚¹ãƒˆé …ç›®æ•°åˆ†æ"]["çµ±è¨ˆå€¤"]
            ext_link_stats = self.analysis_results["basic_statistics"]["å¤–éƒ¨ãƒªãƒ³ã‚¯æ•°åˆ†æ"]["çµ±è¨ˆå€¤"]
            int_link_stats = self.analysis_results["basic_statistics"]["å†…éƒ¨ãƒªãƒ³ã‚¯æ•°åˆ†æ"]["çµ±è¨ˆå€¤"]
            
            stats_summary = f"""
åŸºæœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„çµ±è¨ˆ:
- å¹³å‡æ–‡å­—æ•°: {char_stats['å¹³å‡å€¤']:.0f}æ–‡å­— (ç¯„å›²: {char_stats['æœ€å°å€¤']:.0f}ã€œ{char_stats['æœ€å¤§å€¤']:.0f}æ–‡å­—)
- å¹³å‡ç”»åƒæ•°: {image_stats['å¹³å‡å€¤']:.1f}å€‹ (ç¯„å›²: {image_stats['æœ€å°å€¤']:.0f}ã€œ{image_stats['æœ€å¤§å€¤']:.0f}å€‹)

ãƒãƒ«ãƒãƒ¡ãƒ‡ã‚£ã‚¢æˆ¦ç•¥çµ±è¨ˆ:
- å¹³å‡å‹•ç”»æ•°: {video_stats['å¹³å‡å€¤']:.1f}å€‹ (ç¯„å›²: {video_stats['æœ€å°å€¤']:.0f}ã€œ{video_stats['æœ€å¤§å€¤']:.0f}å€‹)
- å¹³å‡ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {table_stats['å¹³å‡å€¤']:.1f}å€‹ (ç¯„å›²: {table_stats['æœ€å°å€¤']:.0f}ã€œ{table_stats['æœ€å¤§å€¤']:.0f}å€‹)
- å¹³å‡ãƒªã‚¹ãƒˆé …ç›®æ•°: {list_stats['å¹³å‡å€¤']:.1f}é …ç›® (ç¯„å›²: {list_stats['æœ€å°å€¤']:.0f}ã€œ{list_stats['æœ€å¤§å€¤']:.0f}é …ç›®)

ãƒªãƒ³ã‚¯æˆ¦ç•¥çµ±è¨ˆ:
- å¹³å‡å¤–éƒ¨ãƒªãƒ³ã‚¯æ•°: {ext_link_stats['å¹³å‡å€¤']:.1f}å€‹ (ä¿¡é ¼æ€§ãƒ»æ¨©å¨æ€§æŒ‡æ¨™)
- å¹³å‡å†…éƒ¨ãƒªãƒ³ã‚¯æ•°: {int_link_stats['å¹³å‡å€¤']:.1f}å€‹ (ã‚µã‚¤ãƒˆå›éŠæ€§æŒ‡æ¨™)
"""

        if "heading_structure" in self.analysis_results:
            heading_stats = self.analysis_results["heading_structure"]
            total_dist = heading_stats.get("å…¨è¨˜äº‹ã§ã®è¦‹å‡ºã—ãƒ¬ãƒ™ãƒ«åˆ¥ç·æ•°", {})
            stats_summary += f"""
è¦‹å‡ºã—ä½¿ç”¨çŠ¶æ³:
- H1: {total_dist.get('h1', 0)}å›
- H2: {total_dist.get('h2', 0)}å›  
- H3: {total_dist.get('h3', 0)}å›
- H4: {total_dist.get('h4', 0)}å›
- H5: {total_dist.get('h5', 0)}å›
- H6: {total_dist.get('h6', 0)}å›
"""

        # â˜… ãƒãƒ«ãƒãƒ¡ãƒ‡ã‚£ã‚¢ãƒ»E-E-A-Tåˆ†æçµæœã‚‚è¿½åŠ 
        if "multimedia_strategy" in self.analysis_results:
            multimedia = self.analysis_results["multimedia_strategy"]
            stats_summary += f"""
ãƒãƒ«ãƒãƒ¡ãƒ‡ã‚£ã‚¢æˆ¦ç•¥ãƒ‘ã‚¿ãƒ¼ãƒ³:
- å‹•ç”»æ¡ç”¨è¨˜äº‹: {multimedia['å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æ']['æ¡ç”¨ç‡']}
- ãƒ†ãƒ¼ãƒ–ãƒ«æ¡ç”¨è¨˜äº‹: {multimedia['ãƒ†ãƒ¼ãƒ–ãƒ«æ´»ç”¨åˆ†æ']['æ¡ç”¨ç‡']}
- ãƒªã‚¹ãƒˆæ§‹é€ æ¡ç”¨è¨˜äº‹: {multimedia['ãƒªã‚¹ãƒˆæ§‹é€ åˆ†æ']['æ¡ç”¨ç‡']}
"""

        if "eeat_factors" in self.analysis_results:
            eeat = self.analysis_results["eeat_factors"]
            stats_summary += f"""
E-E-A-Tè¦å› çµ±è¨ˆ:
- è‘—è€…æƒ…å ±æ˜è¨˜: {eeat['å°‚é–€æ€§ãƒ»æ¨©å¨æ€§åˆ†æ']['è‘—è€…æƒ…å ±æ˜è¨˜ç‡']}
- å¤–éƒ¨ãƒªãƒ³ã‚¯æ´»ç”¨: {eeat['ä¿¡é ¼æ€§åˆ†æ']['å¤–éƒ¨ãƒªãƒ³ã‚¯æ¡ç”¨ç‡']}
- æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿æ´»ç”¨: {eeat['æŠ€è¡“çš„ä¿¡é ¼æ€§åˆ†æ']['æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿æ¡ç”¨ç‡']}
- E-E-A-Tç·åˆã‚¹ã‚³ã‚¢: {eeat['E-E-A-Tç·åˆè©•ä¾¡']['ç·åˆã‚¹ã‚³ã‚¢']}
"""

        # â˜… é »å‡ºè¦‹å‡ºã—æƒ…å ±ã‚‚stats_summaryã«è¿½åŠ 
        if "ã‚¨ãƒ©ãƒ¼" not in frequent_headings_analysis:
            exact_frequent = frequent_headings_analysis.get("å®Œå…¨ä¸€è‡´é »å‡ºè¦‹å‡ºã—", {})
            similarity_groups = frequent_headings_analysis.get("é¡ä¼¼è¦‹å‡ºã—ã‚°ãƒ«ãƒ¼ãƒ—", {})
            stats_summary += f"""
é »å‡ºè¦‹å‡ºã—åˆ†æ:
- å®Œå…¨ä¸€è‡´é »å‡ºè¦‹å‡ºã—: {exact_frequent.get('é »å‡ºè¦‹å‡ºã—ç·æ•°', 0)}ç¨®é¡
- é¡ä¼¼è¦‹å‡ºã—ã‚°ãƒ«ãƒ¼ãƒ—: {similarity_groups.get('é¡ä¼¼ã‚°ãƒ«ãƒ¼ãƒ—ç·æ•°', 0)}ã‚°ãƒ«ãƒ¼ãƒ—
- é‡è¤‡è¦‹å‡ºã—ç‡: {frequent_headings_analysis.get('å…¨è¦‹å‡ºã—çµ±è¨ˆ', {}).get('é‡è¤‡è¦‹å‡ºã—æ•°', 0)}/{frequent_headings_analysis.get('å…¨è¦‹å‡ºã—çµ±è¨ˆ', {}).get('ç·è¦‹å‡ºã—æ•°', 1)}
"""

        # â˜… ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã«é€ä¿¡ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°ã‚’è¡¨ç¤º
        print("\n" + "="*60)
        print("ğŸ” Gemini APIã«é€ä¿¡ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°:")
        print("="*60)
        print(f"ğŸ“‹ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæƒ…å ±:")
        print(f"   â€¢ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {target_keyword}")
        print(f"   â€¢ è¨˜äº‹ç›®çš„: {article_purpose}")
        print(f"   â€¢ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆèª­è€…: {target_audience}")
        
        print(f"\nğŸ“Š åŸºæœ¬çµ±è¨ˆæƒ…å ±:")
        if "basic_statistics" in self.analysis_results:
            char_stats = self.analysis_results["basic_statistics"]["æ–‡å­—æ•°åˆ†æ"]["çµ±è¨ˆå€¤"]
            print(f"   â€¢ å¹³å‡æ–‡å­—æ•°: {char_stats['å¹³å‡å€¤']:.0f}æ–‡å­—")
            print(f"   â€¢ æ–‡å­—æ•°ç¯„å›²: {char_stats['æœ€å°å€¤']:.0f}ã€œ{char_stats['æœ€å¤§å€¤']:.0f}æ–‡å­—")
        
        print(f"\nğŸ—ï¸  è¦‹å‡ºã—æ§‹é€ æƒ…å ±:")
        total_headings = sum(len(comp["è¦‹å‡ºã—æ§‹é€ "]) for comp in all_competitor_headings)
        print(f"   â€¢ åˆ†æè¨˜äº‹æ•°: {len(all_competitor_headings)}è¨˜äº‹")
        print(f"   â€¢ ç·è¦‹å‡ºã—æ•°: {total_headings}å€‹")
        
        if "heading_structure" in self.analysis_results:
            heading_stats = self.analysis_results["heading_structure"]
            total_dist = heading_stats.get("å…¨è¨˜äº‹ã§ã®è¦‹å‡ºã—ãƒ¬ãƒ™ãƒ«åˆ¥ç·æ•°", {})
            for level in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                count = total_dist.get(level, 0)
                if count > 0:
                    print(f"   â€¢ {level.upper()}: {count}å€‹")
        
        print(f"\nğŸ”„ é »å‡ºè¦‹å‡ºã—åˆ†æçµæœ:")
        if "ã‚¨ãƒ©ãƒ¼" not in frequent_headings_analysis:
            exact_frequent = frequent_headings_analysis.get("å®Œå…¨ä¸€è‡´é »å‡ºè¦‹å‡ºã—", {})
            similarity_groups = frequent_headings_analysis.get("é¡ä¼¼è¦‹å‡ºã—ã‚°ãƒ«ãƒ¼ãƒ—", {})
            stats = frequent_headings_analysis.get("å…¨è¦‹å‡ºã—çµ±è¨ˆ", {})
            
            print(f"   â€¢ ç·è¦‹å‡ºã—æ•°: {stats.get('ç·è¦‹å‡ºã—æ•°', 0)}å€‹")
            print(f"   â€¢ ä¸€æ„è¦‹å‡ºã—æ•°: {stats.get('ä¸€æ„è¦‹å‡ºã—æ•°', 0)}å€‹")
            print(f"   â€¢ é‡è¤‡è¦‹å‡ºã—æ•°: {stats.get('é‡è¤‡è¦‹å‡ºã—æ•°', 0)}å€‹")
            print(f"   â€¢ å®Œå…¨ä¸€è‡´é »å‡ºè¦‹å‡ºã—: {exact_frequent.get('é »å‡ºè¦‹å‡ºã—ç·æ•°', 0)}ç¨®é¡")
            print(f"   â€¢ é¡ä¼¼è¦‹å‡ºã—ã‚°ãƒ«ãƒ¼ãƒ—: {similarity_groups.get('é¡ä¼¼ã‚°ãƒ«ãƒ¼ãƒ—ç·æ•°', 0)}ã‚°ãƒ«ãƒ¼ãƒ—")
            
            # ãƒˆãƒƒãƒ—5ã®é »å‡ºè¦‹å‡ºã—ã‚’è¡¨ç¤º
            top_frequent = exact_frequent.get("ãƒˆãƒƒãƒ—20", [])[:5]
            if top_frequent:
                print(f"   â€¢ ãƒˆãƒƒãƒ—5é »å‡ºè¦‹å‡ºã—:")
                for i, (text, count) in enumerate(top_frequent):
                    print(f"     {i+1}. ã€Œ{text}ã€({count}å›)")
            
            # ãƒˆãƒƒãƒ—3ã®é¡ä¼¼ã‚°ãƒ«ãƒ¼ãƒ—ã‚’è¡¨ç¤º
            top_groups = similarity_groups.get("ãƒˆãƒƒãƒ—10ã‚°ãƒ«ãƒ¼ãƒ—", [])[:3]
            if top_groups:
                print(f"   â€¢ ãƒˆãƒƒãƒ—3é¡ä¼¼ã‚°ãƒ«ãƒ¼ãƒ—:")
                for i, group in enumerate(top_groups):
                    base_text = group.get('ãƒ™ãƒ¼ã‚¹è¦‹å‡ºã—', '')
                    group_size = group.get('é¡ä¼¼ã‚°ãƒ«ãƒ¼ãƒ—ã‚µã‚¤ã‚º', 0)
                    print(f"     {i+1}. ã€Œ{base_text}ã€é¡ä¼¼ã‚°ãƒ«ãƒ¼ãƒ— ({group_size}å€‹)")
        else:
            print(f"   âŒ é »å‡ºè¦‹å‡ºã—åˆ†æã‚¨ãƒ©ãƒ¼: {frequent_headings_analysis.get('ã‚¨ãƒ©ãƒ¼', 'Unknown')}")
        
        print("="*60)

        # â˜… å¼·åŒ–ã•ã‚ŒãŸGemini APIãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
        prompt = f"""
ã‚ãªãŸã¯ã€SEOå°‚é–€ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æˆ¦ç•¥ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚ä»¥ä¸‹ã®ç«¶åˆè¨˜äº‹åˆ†æçµæœã‚’åŸºã«ã€æœ€é©ãªè¦‹å‡ºã—æ§‹é€ ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚

ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæƒ…å ±ã€‘
- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {target_keyword}
- è¨˜äº‹ã®ç›®çš„: {article_purpose}
- ã‚¿ãƒ¼ã‚²ãƒƒãƒˆèª­è€…: {target_audience}

ã€ç«¶åˆè¨˜äº‹ã®çµ±è¨ˆæƒ…å ±ã€‘
{stats_summary}

ã€é »å‡ºè¦‹å‡ºã—ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã€‘
{json.dumps(frequent_headings_analysis, ensure_ascii=False, indent=2)}

ã€ç«¶åˆè¨˜äº‹ã®è©³ç´°åˆ†æãƒ‡ãƒ¼ã‚¿ã€‘
{json.dumps(all_competitor_headings, ensure_ascii=False, indent=2)}

ã€ææ¡ˆè¦ä»¶ã€‘
1. ä¸Šè¨˜ã®ç«¶åˆè¨˜äº‹ã‚’åˆ†æã—ã€å¿…é ˆã¨ã¿ã‚‰ã‚Œã‚‹è¦‹å‡ºã—ãƒˆãƒ”ãƒƒã‚¯ã‚’ç‰¹å®šã—ã¦ãã ã•ã„
2. é »å‡ºè¦‹å‡ºã—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰é‡è¦ãªãƒˆãƒ”ãƒƒã‚¯ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„
3. ç«¶åˆè¨˜äº‹ã«ãªã„ç‹¬è‡ªæ€§ã®ã‚ã‚‹è¦‹å‡ºã—ã‚’ææ¡ˆã—ã¦ãã ã•ã„  
4. SEOåŠ¹æœã‚’æœ€å¤§åŒ–ã™ã‚‹è¦‹å‡ºã—æ§‹é€ ã‚’è¨­è¨ˆã—ã¦ãã ã•ã„
5. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ¤œç´¢æ„å›³ã«å¿œãˆã‚‹åŒ…æ‹¬çš„ãªæ§‹æˆã‚’ææ¡ˆã—ã¦ãã ã•ã„
6. â˜… ãƒãƒ«ãƒãƒ¡ãƒ‡ã‚£ã‚¢æˆ¦ç•¥ã‚‚è€ƒæ…®ã—ã¦ãã ã•ã„ï¼ˆå‹•ç”»ãƒ»ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ»ãƒªã‚¹ãƒˆã®æ´»ç”¨ï¼‰
7. â˜… E-E-A-Tè¦å› ã‚‚è€ƒæ…®ã—ã¦ãã ã•ã„ï¼ˆè‘—è€…æƒ…å ±ãƒ»å¤–éƒ¨ãƒªãƒ³ã‚¯ãƒ»æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ï¼‰
8. é©åˆ‡ãªæ–‡å­—æ•°ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚‚ä½µã›ã¦ææ¡ˆã—ã¦ãã ã•ã„

ã€å‡ºåŠ›å½¢å¼ã€‘
ä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š

{{
  "ææ¡ˆã‚µãƒãƒªãƒ¼": {{
    "ç«¶åˆåˆ†æã®è¦ç‚¹": "string",
    "é »å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç‰¹å¾´": "string",
    "ææ¡ˆã®ç‰¹å¾´": "string", 
    "SEOæˆ¦ç•¥": "string"
  }},
  "æ¨å¥¨è¦‹å‡ºã—æ§‹é€ ": [
    {{
      "ãƒ¬ãƒ™ãƒ«": 1,
      "è¦‹å‡ºã—ãƒ†ã‚­ã‚¹ãƒˆ": "string",
      "ç›®çš„": "introduction|body|conclusion|faq|references",
      "æ¨å¥¨æ–‡å­—æ•°": number,
      "é¸å®šç†ç”±": "string",
      "ç«¶åˆã§ã®ä½¿ç”¨çŠ¶æ³": "string",
      "é »å‡ºåº¦": "é«˜|ä¸­|ä½|ç‹¬è‡ª"
    }}
  ],
  "ç‹¬è‡ªæ€§ã®ã‚ã‚‹ææ¡ˆ": [
    {{
      "è¦‹å‡ºã—ãƒ†ã‚­ã‚¹ãƒˆ": "string", 
      "ãƒ¬ãƒ™ãƒ«": number,
      "å·®åˆ¥åŒ–ãƒã‚¤ãƒ³ãƒˆ": "string",
      "æœŸå¾…åŠ¹æœ": "string"
    }}
  ],
  "è¨˜äº‹å…¨ä½“ã®æ¨å¥¨ä»•æ§˜": {{
    "æ¨å¥¨ç·æ–‡å­—æ•°": number,
    "æ¨å¥¨è¦‹å‡ºã—æ•°": number,
    "æ¨å¥¨ç”»åƒæ•°": number,
    "æ¨å¥¨å‹•ç”»æ•°": number,
    "æ¨å¥¨ãƒ†ãƒ¼ãƒ–ãƒ«æ•°": number,
    "æ¨å¥¨ãƒªã‚¹ãƒˆé …ç›®æ•°": number,
    "æ¨å¥¨å¤–éƒ¨ãƒªãƒ³ã‚¯æ•°": number,
    "æ¨å¥¨å†…éƒ¨ãƒªãƒ³ã‚¯æ•°": number,
    "ä¸»è¦ãªSEOãƒã‚¤ãƒ³ãƒˆ": ["string1", "string2", "string3"]
  }},
  "E-E-A-Tæˆ¦ç•¥ææ¡ˆ": {{
    "è‘—è€…æƒ…å ±ã®æ‰±ã„": "string",
    "å¤–éƒ¨ãƒªãƒ³ã‚¯æˆ¦ç•¥": "string", 
    "æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿æ´»ç”¨": "string",
    "æ—¥ä»˜æƒ…å ±ã®æ˜è¨˜": "string"
  }},
  "ãƒãƒ«ãƒãƒ¡ãƒ‡ã‚£ã‚¢æˆ¦ç•¥ææ¡ˆ": {{
    "å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æˆ¦ç•¥": "string",
    "ãƒ†ãƒ¼ãƒ–ãƒ«æ´»ç”¨æˆ¦ç•¥": "string",
    "ãƒªã‚¹ãƒˆæ§‹é€ æˆ¦ç•¥": "string",
    "ç”»åƒé…ç½®æˆ¦ç•¥": "string"
  }}
}}

å¿…ãšã“ã®JSONå½¢å¼ã§å›ç­”ã—ã€ä»–ã®å½¢å¼ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚
"""

        print(f"\nğŸš€ Gemini APIã«åˆ†æã‚’ä¾é ¼ã—ã¾ã™...")
        print(f"   ğŸ“¤ é€ä¿¡ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: ç´„{len(prompt):,}æ–‡å­—")
        print(f"   ğŸ¯ ç«¶åˆè¨˜äº‹æ•°: {len(all_competitor_headings)}è¨˜äº‹")
        print(f"   ğŸ“Š é »å‡ºè¦‹å‡ºã—åˆ†æçµæœã‚‚å«ã‚€")
        
        try:
            generation_config = genai.types.GenerationConfig(
                response_mime_type="application/json",
                temperature=0.3
            )
            
            response = await model.generate_content_async(
                contents=[prompt],
                generation_config=generation_config
            )
            
            if not response.text:
                return {
                    "ã‚¨ãƒ©ãƒ¼": "Gemini APIã‹ã‚‰ã®å¿œç­”ãŒç©ºã§ã—ãŸã€‚",
                    "error": "Empty response from Gemini API."
                }

            try:
                gemini_result = json.loads(response.text)
            except json.JSONDecodeError as e:
                return {
                    "ã‚¨ãƒ©ãƒ¼": f"Gemini APIã®å¿œç­”ã‚’JSONã¨ã—ã¦è§£æã§ãã¾ã›ã‚“ã§ã—ãŸ: {str(e)}",
                    "error": f"Failed to parse Gemini API response as JSON: {str(e)}",
                    "ç”Ÿã®å¿œç­”": response.text[:500]
                }

            # çµæœã®å¾Œå‡¦ç†ã¨æ—¥æœ¬èªåŒ–
            result = {
                "åˆ†æå®Ÿè¡Œæ—¥æ™‚": datetime.datetime.now().isoformat(),
                "analysis_timestamp": datetime.datetime.now().isoformat(),
                "å…¥åŠ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿": {
                    "ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": target_keyword,
                    "target_keyword": target_keyword,
                    "è¨˜äº‹ç›®çš„": article_purpose,
                    "article_purpose": article_purpose,
                    "ã‚¿ãƒ¼ã‚²ãƒƒãƒˆèª­è€…": target_audience,
                    "target_audience": target_audience
                },
                "input_parameters": {
                    "target_keyword": target_keyword,
                    "article_purpose": article_purpose,
                    "target_audience": target_audience
                },
                "ç«¶åˆè¨˜äº‹åˆ†æã‚µãƒãƒªãƒ¼": {
                    "åˆ†æè¨˜äº‹æ•°": len(self.articles),
                    "analyzed_articles": len(self.articles),
                    "ç·è¦‹å‡ºã—æ•°": sum(len(comp["è¦‹å‡ºã—æ§‹é€ "]) for comp in all_competitor_headings),
                    "total_headings": sum(len(comp["è¦‹å‡ºã—æ§‹é€ "]) for comp in all_competitor_headings)
                },
                "competitor_analysis_summary": {
                    "analyzed_articles": len(self.articles),
                    "total_headings": sum(len(comp["è¦‹å‡ºã—æ§‹é€ "]) for comp in all_competitor_headings)
                },
                "é »å‡ºè¦‹å‡ºã—åˆ†æçµæœ": frequent_headings_analysis,  # â˜… é »å‡ºè¦‹å‡ºã—åˆ†æçµæœã‚’è¿½åŠ 
                "frequent_headings_analysis": frequent_headings_analysis,
                "Geminiåˆ†æçµæœ": gemini_result,
                "gemini_analysis": gemini_result,
                "åˆ†æã‚µãƒãƒªãƒ¼": f"Gemini APIã«ã‚ˆã‚‹æœ€é©è¦‹å‡ºã—æ§‹é€ ã®ææ¡ˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{target_keyword}ã€ã«å¯¾ã™ã‚‹{len(self.articles)}è¨˜äº‹ã®åˆ†æçµæœã§ã™ã€‚",
                "summary_jp": f"Gemini APIã«ã‚ˆã‚‹æœ€é©è¦‹å‡ºã—æ§‹é€ ã®ææ¡ˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{target_keyword}ã€ã«å¯¾ã™ã‚‹{len(self.articles)}è¨˜äº‹ã®åˆ†æçµæœã§ã™ã€‚"
            }

            print("âœ… æœ€é©è¦‹å‡ºã—æ§‹é€ ã®ææ¡ˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
            
            # analysis_resultsã«ã‚‚ä¿å­˜
            self.analysis_results["optimal_headings_suggestion"] = result
            
            return result

        except Exception as e:
            error_msg = f"Gemini APIå‘¼ã³å‡ºã—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            print(f"âŒ {error_msg}")
            return {
                "ã‚¨ãƒ©ãƒ¼": error_msg,
                "error": error_msg,
                "å…¥åŠ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿": {
                    "target_keyword": target_keyword,
                    "article_purpose": article_purpose,
                    "target_audience": target_audience
                }
            }

    def analyze_multimedia_strategy(self) -> Dict[str, Any]:
        """
        ç«¶åˆè¨˜äº‹ã®ãƒãƒ«ãƒãƒ¡ãƒ‡ã‚£ã‚¢æˆ¦ç•¥ã‚’åˆ†æã™ã‚‹
        å‹•ç”»ã€ãƒ†ãƒ¼ãƒ–ãƒ«ã€ãƒªã‚¹ãƒˆã®ä½¿ç”¨çŠ¶æ³ã‹ã‚‰ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæˆ¦ç•¥ã‚’è§£æ
        """
        print("ãƒãƒ«ãƒãƒ¡ãƒ‡ã‚£ã‚¢æˆ¦ç•¥ã®åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
        
        if not self.articles:
            return {
                "ã‚¨ãƒ©ãƒ¼": "åˆ†æå¯¾è±¡ã®è¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“ã€‚",
                "error": "No articles available for analysis."
            }

        # ãƒ‡ãƒ¼ã‚¿åé›†
        video_counts = [getattr(article, 'video_count', 0) for article in self.articles]
        table_counts = [getattr(article, 'table_count', 0) for article in self.articles]
        list_item_counts = [getattr(article, 'list_item_count', 0) for article in self.articles]
        
        # çµ±è¨ˆåˆ†æ
        video_stats = self._analyze_distribution(video_counts, "video_count")
        table_stats = self._analyze_distribution(table_counts, "table_count")
        list_item_stats = self._analyze_distribution(list_item_counts, "list_item_count")
        
        # æˆ¦ç•¥çš„åˆ†æ
        articles_with_video = sum(1 for count in video_counts if count > 0)
        articles_with_tables = sum(1 for count in table_counts if count > 0)
        articles_with_lists = sum(1 for count in list_item_counts if count > 0)
        
        total_articles = len(self.articles)
        video_adoption_rate = (articles_with_video / total_articles) * 100
        table_adoption_rate = (articles_with_tables / total_articles) * 100
        list_adoption_rate = (articles_with_lists / total_articles) * 100
        
        # æˆ¦ç•¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç‰¹å®š
        multimedia_strategies = []
        
        if video_adoption_rate >= 70:
            multimedia_strategies.append("å‹•ç”»é‡è¦–æˆ¦ç•¥ãŒä¸»æµ")
        elif video_adoption_rate >= 30:
            multimedia_strategies.append("å‹•ç”»ã‚’éƒ¨åˆ†çš„ã«æ´»ç”¨")
        else:
            multimedia_strategies.append("å‹•ç”»æ´»ç”¨ã¯å°‘æ•°æ´¾")
            
        if table_adoption_rate >= 70:
            multimedia_strategies.append("ãƒ†ãƒ¼ãƒ–ãƒ«æ´»ç”¨ãŒä¸€èˆ¬çš„ï¼ˆå¼·èª¿ã‚¹ãƒ‹ãƒšãƒƒãƒˆå¯¾ç­–ï¼‰")
        elif table_adoption_rate >= 30:
            multimedia_strategies.append("ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’é©åº¦ã«æ´»ç”¨")
        else:
            multimedia_strategies.append("ãƒ†ãƒ¼ãƒ–ãƒ«æ´»ç”¨ã®æ©Ÿä¼šã‚ã‚Š")
            
        if list_adoption_rate >= 80:
            multimedia_strategies.append("ãƒªã‚¹ãƒˆå½¢å¼ãŒæ¨™æº–çš„")
        elif list_adoption_rate >= 50:
            multimedia_strategies.append("ãƒªã‚¹ãƒˆå½¢å¼ã‚’ç©æ¥µæ´»ç”¨")
        else:
            multimedia_strategies.append("ãƒªã‚¹ãƒˆæ´»ç”¨ã§å·®åˆ¥åŒ–å¯èƒ½")

        # æ¨å¥¨æˆ¦ç•¥ã®ç”Ÿæˆ
        recommendations = []
        
        if video_stats['å¹³å‡å€¤'] > 0:
            recommendations.append(f"å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„: å¹³å‡{video_stats['å¹³å‡å€¤']:.1f}å€‹ã‚’ç›®æ¨™ã«å‹•ç”»åŸ‹ã‚è¾¼ã¿ã‚’æ¤œè¨")
        else:
            recommendations.append("å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„: ç«¶åˆãŒã»ã¼æœªæ´»ç”¨ã®ãŸã‚ã€å‹•ç”»ã§å¤§ããå·®åˆ¥åŒ–å¯èƒ½")
            
        if table_stats['å¹³å‡å€¤'] >= 2:
            recommendations.append(f"ãƒ†ãƒ¼ãƒ–ãƒ«æ´»ç”¨: å¹³å‡{table_stats['å¹³å‡å€¤']:.1f}å€‹ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã§æƒ…å ±æ•´ç†ã‚’å¼·åŒ–")
        else:
            recommendations.append("ãƒ†ãƒ¼ãƒ–ãƒ«æ´»ç”¨: æƒ…å ±ã‚’æ•´ç†ã—ã¦å¼·èª¿ã‚¹ãƒ‹ãƒšãƒƒãƒˆç²å¾—ã‚’ç‹™ã†")
            
        if list_item_stats['å¹³å‡å€¤'] >= 10:
            recommendations.append(f"ãƒªã‚¹ãƒˆæ§‹é€ : å¹³å‡{list_item_stats['å¹³å‡å€¤']:.1f}é …ç›®ã®ç¶²ç¾…æ€§ã‚’ç›®æŒ‡ã™")
        else:
            recommendations.append("ãƒªã‚¹ãƒˆæ§‹é€ : ã‚ˆã‚Šè©³ç´°ãªé …ç›®ç«‹ã¦ã§ç¶²ç¾…æ€§ã‚’ã‚¢ãƒ”ãƒ¼ãƒ«")

        result = {
            "åˆ†æå¯¾è±¡è¨˜äº‹æ•°": total_articles,
            "å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æ": {
                "çµ±è¨ˆæƒ…å ±": video_stats,
                "æ¡ç”¨ç‡": f"{video_adoption_rate:.1f}% ({articles_with_video}/{total_articles}è¨˜äº‹)",
                "æˆ¦ç•¥çš„è©•ä¾¡": "é«˜ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæˆ¦ç•¥" if video_adoption_rate >= 50 else "å‹•ç”»æ´»ç”¨ã§å·®åˆ¥åŒ–ã®ãƒãƒ£ãƒ³ã‚¹"
            },
            "ãƒ†ãƒ¼ãƒ–ãƒ«æ´»ç”¨åˆ†æ": {
                "çµ±è¨ˆæƒ…å ±": table_stats,
                "æ¡ç”¨ç‡": f"{table_adoption_rate:.1f}% ({articles_with_tables}/{total_articles}è¨˜äº‹)",
                "æˆ¦ç•¥çš„è©•ä¾¡": "å¼·èª¿ã‚¹ãƒ‹ãƒšãƒƒãƒˆå¯¾ç­–ãŒæ¨™æº–" if table_adoption_rate >= 60 else "ãƒ†ãƒ¼ãƒ–ãƒ«æ´»ç”¨ã§æ¤œç´¢çµæœå‘ä¸Šã®æ©Ÿä¼š"
            },
            "ãƒªã‚¹ãƒˆæ§‹é€ åˆ†æ": {
                "çµ±è¨ˆæƒ…å ±": list_item_stats,
                "æ¡ç”¨ç‡": f"{list_adoption_rate:.1f}% ({articles_with_lists}/{total_articles}è¨˜äº‹)",
                "æˆ¦ç•¥çš„è©•ä¾¡": "ç¶²ç¾…æ€§é‡è¦–ãŒä¸»æµ" if list_adoption_rate >= 70 else "ãƒªã‚¹ãƒˆæ´»ç”¨ã§èª­ã¿ã‚„ã™ã•å‘ä¸Šã®ä½™åœ°"
            },
            "ç«¶åˆæˆ¦ç•¥ãƒ‘ã‚¿ãƒ¼ãƒ³": multimedia_strategies,
            "æ¨å¥¨ãƒãƒ«ãƒãƒ¡ãƒ‡ã‚£ã‚¢æˆ¦ç•¥": recommendations,
            "æˆ¦ç•¥ã‚µãƒãƒªãƒ¼": f"ãƒãƒ«ãƒãƒ¡ãƒ‡ã‚£ã‚¢åˆ†æå®Œäº†: å‹•ç”»{video_adoption_rate:.0f}%ã€ãƒ†ãƒ¼ãƒ–ãƒ«{table_adoption_rate:.0f}%ã€ãƒªã‚¹ãƒˆ{list_adoption_rate:.0f}%ã®æ¡ç”¨ç‡",
            "summary_jp": f"ãƒãƒ«ãƒãƒ¡ãƒ‡ã‚£ã‚¢åˆ†æå®Œäº†: å‹•ç”»{video_adoption_rate:.0f}%ã€ãƒ†ãƒ¼ãƒ–ãƒ«{table_adoption_rate:.0f}%ã€ãƒªã‚¹ãƒˆ{list_adoption_rate:.0f}%ã®æ¡ç”¨ç‡"
        }
        
        self.analysis_results["multimedia_strategy"] = result
        print("ãƒãƒ«ãƒãƒ¡ãƒ‡ã‚£ã‚¢æˆ¦ç•¥ã®åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        return result

    def analyze_eeat_factors(self) -> Dict[str, Any]:
        """
        E-E-A-Tï¼ˆExperience, Expertise, Authoritativeness, Trustworthinessï¼‰è¦å› ã‚’åˆ†æã™ã‚‹
        è‘—è€…æƒ…å ±ã€å¤–éƒ¨ãƒªãƒ³ã‚¯ã€å…¬é–‹æ—¥ã€æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ãªã©ã‹ã‚‰ä¿¡é ¼æ€§æŒ‡æ¨™ã‚’è©•ä¾¡
        """
        print("E-E-A-Tè¦å› ã®åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
        
        if not self.articles:
            return {
                "ã‚¨ãƒ©ãƒ¼": "åˆ†æå¯¾è±¡ã®è¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“ã€‚",
                "error": "No articles available for analysis."
            }

        total_articles = len(self.articles)
        
        # 1. Expertise & Authoritativenessï¼ˆå°‚é–€æ€§ãƒ»æ¨©å¨æ€§ï¼‰
        articles_with_author = sum(1 for article in self.articles if getattr(article, 'author_info', None))
        author_coverage_rate = (articles_with_author / total_articles) * 100
        
        # 2. Trustworthinessï¼ˆä¿¡é ¼æ€§ï¼‰- å¤–éƒ¨ãƒªãƒ³ã‚¯åˆ†æ
        external_link_counts = [getattr(article, 'external_link_count', 0) for article in self.articles]
        internal_link_counts = [getattr(article, 'internal_link_count', 0) for article in self.articles]
        
        external_link_stats = self._analyze_distribution(external_link_counts, "external_links")
        internal_link_stats = self._analyze_distribution(internal_link_counts, "internal_links")
        
        articles_with_external_links = sum(1 for count in external_link_counts if count > 0)
        external_link_adoption_rate = (articles_with_external_links / total_articles) * 100
        
        # 3. Experience & Freshnessï¼ˆçµŒé¨“ãƒ»é®®åº¦ï¼‰
        articles_with_publish_date = sum(1 for article in self.articles if getattr(article, 'publish_date', None))
        articles_with_modified_date = sum(1 for article in self.articles if getattr(article, 'modified_date', None))
        
        publish_date_rate = (articles_with_publish_date / total_articles) * 100
        modified_date_rate = (articles_with_modified_date / total_articles) * 100
        
        # 4. Technical Trustworthinessï¼ˆæŠ€è¡“çš„ä¿¡é ¼æ€§ï¼‰- æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿
        articles_with_schema = sum(1 for article in self.articles 
                                 if getattr(article, 'schema_types', []))
        schema_adoption_rate = (articles_with_schema / total_articles) * 100
        
        # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã®ç¨®é¡åˆ†æ
        all_schema_types = []
        for article in self.articles:
            schema_types = getattr(article, 'schema_types', [])
            all_schema_types.extend(schema_types)
        
        schema_counter = Counter(all_schema_types)
        popular_schemas = schema_counter.most_common(5)
        
        # E-E-A-Tç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        eeat_factors = {
            "è‘—è€…æƒ…å ±æ˜è¨˜": author_coverage_rate * 0.25,
            "å¤–éƒ¨ãƒªãƒ³ã‚¯æ´»ç”¨": min(external_link_adoption_rate, 80) * 0.20,  # 80%ã‚’ä¸Šé™
            "æ—¥ä»˜æƒ…å ±æ˜è¨˜": max(publish_date_rate, modified_date_rate) * 0.15,
            "æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿æ´»ç”¨": min(schema_adoption_rate, 90) * 0.20,  # 90%ã‚’ä¸Šé™
            "æƒ…å ±ã®å‚ç…§æ€§": min(external_link_stats['å¹³å‡å€¤'] * 10, 30) * 0.20  # å¹³å‡å¤–éƒ¨ãƒªãƒ³ã‚¯æ•°Ã—10ã€30ã‚’ä¸Šé™
        }
        
        total_eeat_score = sum(eeat_factors.values())
        
        # æˆ¦ç•¥çš„è©•ä¾¡
        eeat_evaluation = []
        if author_coverage_rate >= 70:
            eeat_evaluation.append("è‘—è€…æƒ…å ±ã®æ˜è¨˜ãŒæ¨™æº–çš„")
        elif author_coverage_rate >= 30:
            eeat_evaluation.append("è‘—è€…æƒ…å ±ã®æ˜è¨˜ã¯éƒ¨åˆ†çš„")
        else:
            eeat_evaluation.append("è‘—è€…æƒ…å ±æ˜è¨˜ã§å¤§ããå·®åˆ¥åŒ–å¯èƒ½")
            
        if external_link_adoption_rate >= 70:
            eeat_evaluation.append("å¤–éƒ¨ãƒªãƒ³ã‚¯ã«ã‚ˆã‚‹æ¨©å¨æ€§ã‚¢ãƒ”ãƒ¼ãƒ«ãŒä¸€èˆ¬çš„")
        elif external_link_adoption_rate >= 30:
            eeat_evaluation.append("å¤–éƒ¨ãƒªãƒ³ã‚¯æ´»ç”¨ã¯ä¸­ç¨‹åº¦")
        else:
            eeat_evaluation.append("å¤–éƒ¨ãƒªãƒ³ã‚¯ã§ä¿¡é ¼æ€§å‘ä¸Šã®æ©Ÿä¼šå¤§")
            
        if schema_adoption_rate >= 50:
            eeat_evaluation.append("æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿æ´»ç”¨ãŒé€²ã‚“ã§ã„ã‚‹")
        else:
            eeat_evaluation.append("æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã§æŠ€è¡“çš„å„ªä½æ€§ã‚’ç²å¾—å¯èƒ½")

        # æ¨å¥¨æ–½ç­–
        recommendations = []
        
        if author_coverage_rate < 50:
            recommendations.append("è‘—è€…æƒ…å ±ã®æ˜è¨˜: å°‚é–€æ€§ã‚’ã‚¢ãƒ”ãƒ¼ãƒ«ã—ã¦ä¿¡é ¼æ€§ã‚’å‘ä¸Š")
        
        if external_link_stats['å¹³å‡å€¤'] < 3:
            recommendations.append("å¤–éƒ¨ãƒªãƒ³ã‚¯å¼·åŒ–: ä¿¡é ¼ã§ãã‚‹æƒ…å ±æºã¸ã®å‚ç…§ã‚’3ä»¶ä»¥ä¸Šè¿½åŠ ")
        elif external_link_stats['å¹³å‡å€¤'] > 10:
            recommendations.append("å¤–éƒ¨ãƒªãƒ³ã‚¯æœ€é©åŒ–: éåº¦ãªãƒªãƒ³ã‚¯ã¯é¿ã‘ã€å³é¸ã—ãŸå‚ç…§ã«çµã‚‹")
            
        if publish_date_rate < 30:
            recommendations.append("æ—¥ä»˜æƒ…å ±æ˜è¨˜: å…¬é–‹æ—¥ãƒ»æ›´æ–°æ—¥ã‚’æ˜ç¤ºã—ã¦æƒ…å ±ã®é®®åº¦ã‚’ã‚¢ãƒ”ãƒ¼ãƒ«")
            
        if schema_adoption_rate < 30:
            recommendations.append("æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿å®Ÿè£…: Article, FAQç­‰ã®ã‚¹ã‚­ãƒ¼ãƒã§ãƒªãƒƒãƒãƒªã‚¶ãƒ«ãƒˆå¯¾ç­–")

        result = {
            "åˆ†æå¯¾è±¡è¨˜äº‹æ•°": total_articles,
            "å°‚é–€æ€§ãƒ»æ¨©å¨æ€§åˆ†æ": {
                "è‘—è€…æƒ…å ±æ˜è¨˜ç‡": f"{author_coverage_rate:.1f}% ({articles_with_author}/{total_articles}è¨˜äº‹)",
                "è‘—è€…æƒ…å ±ä¸€è¦§": [getattr(article, 'author_info', 'ãªã—') for article in self.articles],
                "æˆ¦ç•¥çš„è©•ä¾¡": "æ¨©å¨æ€§ã‚¢ãƒ”ãƒ¼ãƒ«ãŒæ¨™æº–" if author_coverage_rate >= 60 else "å°‚é–€æ€§ã‚¢ãƒ”ãƒ¼ãƒ«ã§å·®åˆ¥åŒ–ã®ãƒãƒ£ãƒ³ã‚¹"
            },
            "ä¿¡é ¼æ€§åˆ†æ": {
                "å¤–éƒ¨ãƒªãƒ³ã‚¯çµ±è¨ˆ": external_link_stats,
                "å¤–éƒ¨ãƒªãƒ³ã‚¯æ¡ç”¨ç‡": f"{external_link_adoption_rate:.1f}% ({articles_with_external_links}/{total_articles}è¨˜äº‹)",
                "å†…éƒ¨ãƒªãƒ³ã‚¯çµ±è¨ˆ": internal_link_stats,
                "æˆ¦ç•¥çš„è©•ä¾¡": "å‚ç…§ã«ã‚ˆã‚‹ä¿¡é ¼æ€§ãŒç¢ºç«‹" if external_link_adoption_rate >= 70 else "å¤–éƒ¨å‚ç…§ã§ä¿¡é ¼æ€§å‘ä¸Šã®ä½™åœ°"
            },
            "é®®åº¦ãƒ»çµŒé¨“åˆ†æ": {
                "å…¬é–‹æ—¥æ˜è¨˜ç‡": f"{publish_date_rate:.1f}% ({articles_with_publish_date}/{total_articles}è¨˜äº‹)",
                "æ›´æ–°æ—¥æ˜è¨˜ç‡": f"{modified_date_rate:.1f}% ({articles_with_modified_date}/{total_articles}è¨˜äº‹)",
                "æˆ¦ç•¥çš„è©•ä¾¡": "æƒ…å ±é®®åº¦ã®é€æ˜æ€§ãŒé«˜ã„" if max(publish_date_rate, modified_date_rate) >= 50 else "æ—¥ä»˜æ˜è¨˜ã§é®®åº¦ã‚¢ãƒ”ãƒ¼ãƒ«ã®æ©Ÿä¼š"
            },
            "æŠ€è¡“çš„ä¿¡é ¼æ€§åˆ†æ": {
                "æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿æ¡ç”¨ç‡": f"{schema_adoption_rate:.1f}% ({articles_with_schema}/{total_articles}è¨˜äº‹)",
                "äººæ°—ã‚¹ã‚­ãƒ¼ãƒã‚¿ã‚¤ãƒ—": [{"ã‚¿ã‚¤ãƒ—": schema, "ä½¿ç”¨å›æ•°": count} for schema, count in popular_schemas],
                "æˆ¦ç•¥çš„è©•ä¾¡": "æŠ€è¡“SEOå¯¾ç­–ãŒé€²ã‚“ã§ã„ã‚‹" if schema_adoption_rate >= 40 else "æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã§æŠ€è¡“çš„å„ªä½æ€§ã®æ©Ÿä¼š"
            },
            "E-E-A-Tç·åˆè©•ä¾¡": {
                "ç·åˆã‚¹ã‚³ã‚¢": f"{total_eeat_score:.1f}/100",
                "è¦å› åˆ¥ã‚¹ã‚³ã‚¢": eeat_factors,
                "è©•ä¾¡ãƒ¬ãƒ™ãƒ«": "å„ªç§€" if total_eeat_score >= 70 else "è‰¯å¥½" if total_eeat_score >= 50 else "æ”¹å–„ä½™åœ°ã‚ã‚Š"
            },
            "ç«¶åˆE-E-A-Tæˆ¦ç•¥": eeat_evaluation,
            "æ¨å¥¨E-E-A-Tæ–½ç­–": recommendations,
            "æˆ¦ç•¥ã‚µãƒãƒªãƒ¼": f"E-E-A-Tåˆ†æå®Œäº†: ç·åˆã‚¹ã‚³ã‚¢{total_eeat_score:.0f}/100ã€è‘—è€…æ˜è¨˜{author_coverage_rate:.0f}%ã€å¤–éƒ¨ãƒªãƒ³ã‚¯æ´»ç”¨{external_link_adoption_rate:.0f}%",
            "summary_jp": f"E-E-A-Tåˆ†æå®Œäº†: ç·åˆã‚¹ã‚³ã‚¢{total_eeat_score:.0f}/100ã€è‘—è€…æ˜è¨˜{author_coverage_rate:.0f}%ã€å¤–éƒ¨ãƒªãƒ³ã‚¯æ´»ç”¨{external_link_adoption_rate:.0f}%"
        }
        
        self.analysis_results["eeat_factors"] = result
        print("E-E-A-Tè¦å› ã®åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        return result

