import json
import asyncio
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import requests
from bs4 import BeautifulSoup, NavigableString, Tag, PageElement
from serpapi.google_search import GoogleSearch  # type: ignore[import-untyped]
from app.core.config import settings
import urllib.robotparser
from app.infrastructure.gcp_auth import setup_genai_client
from urllib.parse import urlparse
import time # â˜… è¿½åŠ : æ™‚é–“è¨ˆæ¸¬ç”¨
import google.generativeai as genai # â˜… è¿½åŠ : Gemini APIç”¨
import re # â˜… è¿½åŠ : æ­£è¦è¡¨ç¾ç”¨ï¼ˆè‘—è€…æƒ…å ±æŠ½å‡ºãªã©ï¼‰

# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ™‚ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
USER_AGENT = "Mozilla/5.0 (compatible; ShintairikuBot/1.0; +https://shintairiku.com/bot)"


@dataclass
class ScrapedArticle:
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ãŸè¨˜äº‹ã®æƒ…å ±"""
    url: str
    title: str
    headings: List[Dict[str, Any]]  # è¦‹å‡ºã—ã®ãƒªã‚¹ãƒˆ (éšå±¤æ§‹é€ å¯¾å¿œ, char_count_section ã‚’å«ã‚€)
    content: str  # è¨˜äº‹æœ¬æ–‡
    char_count: int  # æ–‡å­—æ•°
    image_count: int  # ç”»åƒæ•°
    source_type: str  # "related_question" ã¾ãŸã¯ "organic_result"
    position: Optional[int] = None  # organic_resultã®å ´åˆã®é †ä½
    question: Optional[str] = None  # related_questionã®å ´åˆã®è³ªå•æ–‡
    # â˜… æ–°ã—ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆåˆ†æãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
    video_count: int = 0  # å‹•ç”»æ•°ï¼ˆiframeå«ã‚€ï¼‰
    table_count: int = 0  # ãƒ†ãƒ¼ãƒ–ãƒ«æ•°
    list_item_count: int = 0  # ãƒªã‚¹ãƒˆé …ç›®ç·æ•°
    external_link_count: int = 0  # å¤–éƒ¨ãƒªãƒ³ã‚¯æ•°
    internal_link_count: int = 0  # å†…éƒ¨ãƒªãƒ³ã‚¯æ•°
    # â˜… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    author_info: Optional[str] = None  # è‘—è€…æƒ…å ±
    publish_date: Optional[str] = None  # å…¬é–‹æ—¥
    modified_date: Optional[str] = None  # æ›´æ–°æ—¥
    schema_types: Optional[List[str]] = None  # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã®ã‚¿ã‚¤ãƒ—ãƒªã‚¹ãƒˆ
    
    def __post_init__(self):
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã®åˆæœŸåŒ–"""
        if self.schema_types is None:
            self.schema_types = []


@dataclass
class SerpAnalysisResult:
    """SerpAPIåˆ†æçµæœ"""
    search_query: str
    total_results: int
    related_questions: List[Dict[str, Any]]
    organic_results: List[Dict[str, Any]]
    scraped_articles: List[ScrapedArticle]
    average_char_count: int
    suggested_target_length: int


class SerpAPIService:
    """SerpAPIã¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ©Ÿèƒ½ã‚’æä¾›ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹"""
    
    USER_AGENT = USER_AGENT # ã‚¯ãƒ©ã‚¹å¤‰æ•°ã¨ã—ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®šç¾©

    def __init__(self):
        # è¨­å®šã‹ã‚‰æ­£ã—ãèª­ã¿è¾¼ã¿
        self.api_key = settings.serpapi_key
        self.robot_parsers: Dict[str, Optional[urllib.robotparser.RobotFileParser]] = {} # robots.txtãƒ‘ãƒ¼ã‚µãƒ¼ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        
    def _ensure_api_key(self):
        """APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ã—ã€ãªã‘ã‚Œã°ä¾‹å¤–ã‚’ç™ºç”Ÿã•ã›ã‚‹"""
        if not self.api_key or self.api_key.strip() == "":
            # è¨­å®šã‚’å†èª­ã¿è¾¼ã¿ã—ã¦ã¿ã‚‹
            from app.core.config import settings
            self.api_key = settings.serpapi_key
            
            if not self.api_key or self.api_key.strip() == "":
                raise ValueError("SERPAPI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.env ãƒ•ã‚¡ã‚¤ãƒ«ã« SERPAPI_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        
    async def analyze_keywords(self, keywords: List[str], num_articles_to_scrape: int = 5) -> SerpAnalysisResult:
        """
        ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åˆ†æã—ã€Googleæ¤œç´¢çµæœã‚’SerpAPIã§å–å¾—ã—ã¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹
        
        Args:
            keywords: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ
            num_articles_to_scrape: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹è¨˜äº‹æ•°ï¼ˆä¸Šä½ã‹ã‚‰ï¼‰
        
        Returns:
            SerpAnalysisResult: åˆ†æçµæœ
        """
        self._ensure_api_key()
        search_query = " ".join(keywords)
        search_results = await self._get_search_results(search_query)
        
        # search_results ãŒã‚¨ãƒ©ãƒ¼ã‚’å«ã‚“ã§ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if "error" in search_results:
            print(f"Cannot proceed to scrape due to SerpAPI error: {search_results.get('error')}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—çµæœã‚„ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§SerpAnalysisResultã‚’è¿”ã™ã‹ã€ä¾‹å¤–ã‚’ç™ºç”Ÿã•ã›ã‚‹
            return SerpAnalysisResult(
                search_query=search_query,
                total_results=0,
                related_questions=[],
                organic_results=[],
                scraped_articles=[],
                average_char_count=3000, # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                suggested_target_length=3300 # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            )

        scraped_articles = await self._scrape_articles(search_results, num_articles_to_scrape)
        
        if scraped_articles:
            average_char_count = sum(article.char_count for article in scraped_articles) // len(scraped_articles)
            suggested_target_length = int(average_char_count * 1.1)
        else:
            average_char_count = 3000
            suggested_target_length = 3300
        
        return SerpAnalysisResult(
            search_query=search_query,
            total_results=search_results.get("search_information", {}).get("total_results", 0),
            related_questions=search_results.get("related_questions", []),
            organic_results=search_results.get("organic_results", []),
            scraped_articles=scraped_articles,
            average_char_count=average_char_count,
            suggested_target_length=suggested_target_length
        )
    
    async def _get_search_results(self, query: str) -> Dict[str, Any]:
        """
        SerpAPIã§Googleæ¤œç´¢çµæœã‚’å–å¾—
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
        
        Returns:
            SerpAPIã®æ¤œç´¢çµæœ
        """
        print(f"Attempting to call actual SerpAPI for query: {query}")
        try:
            results = await self._call_serpapi_real(query)
            if "error" in results:
                print(f"SerpAPI call resulted in an error for query '{query}'. Error: {results['error']}")
                return results 
            print(f"Successfully retrieved real SerpAPI data for query: {query}")
            return results
        except Exception as e:
            print(f"Unexpected error during SerpAPI call for query '{query}'. Exception: {e}")
            return {"error": f"Unexpected exception in _get_search_results: {str(e)}", "query": query}
        
    async def _get_robot_parser(self, base_url: str) -> Optional[urllib.robotparser.RobotFileParser]:
        """æŒ‡å®šã•ã‚ŒãŸãƒ™ãƒ¼ã‚¹URLã®robots.txtã‚’å–å¾—ã—ã¦ãƒ‘ãƒ¼ã‚µãƒ¼ã‚’è¿”ã™ã€‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚‚åˆ©ç”¨ã€‚"""
        if base_url in self.robot_parsers:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒ‘ãƒ¼ã‚µãƒ¼ãŒNoneï¼ˆå–å¾—å¤±æ•—ã‚„å­˜åœ¨ã—ãªã„ï¼‰ã®å ´åˆã‚‚ãã®ã¾ã¾è¿”ã™
            return self.robot_parsers[base_url]

        robots_url = f"{base_url.rstrip('/')}/robots.txt"
        parser = urllib.robotparser.RobotFileParser()
        # parser.set_url(robots_url) # set_urlã¯read()ã®ä¸­ã§å‘¼ã°ã‚Œã‚‹ã®ã§é€šå¸¸ä¸è¦

        try:
            print(f"Fetching robots.txt from: {robots_url}")
            # requests.getã¯ãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ã™ã‚‹ã®ã§ã€éåŒæœŸå®Ÿè¡Œ
            # response = await asyncio.to_thread(
            #     lambda: requests.get(robots_url, headers={"User-Agent": self.USER_AGENT}, timeout=5)
            # )
            # urllib.robotparser.RobotFileParser.read() ã¯å†…éƒ¨ã§ urllib.request ã‚’ä½¿ã†ãŒã€
            # ãã‚Œã‚‚ãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ã™ã‚‹ã®ã§ to_thread ã§åŒ…ã‚€
            
            # parser.read() ã‚’ç›´æ¥éåŒæœŸåŒ–ã™ã‚‹ä»£ã‚ã‚Šã«ã€ã¾ãšrobots.txtã®å†…å®¹ã‚’å–å¾—
            response_content = None
            try:
                response = await asyncio.to_thread(
                    lambda: requests.get(robots_url, headers={"User-Agent": self.USER_AGENT}, timeout=5)
                )
                if response.status_code == 200:
                    response_content = response.text
                else:
                    print(f"Failed to fetch robots.txt from {robots_url}, status: {response.status_code}. Assuming allowed.")
                    self.robot_parsers[base_url] = None 
                    return None
            except Exception as fetch_exc:
                 print(f"Exception fetching robots.txt from {robots_url}: {fetch_exc}. Assuming allowed.")
                 self.robot_parsers[base_url] = None
                 return None

            if response_content:
                # parser.parse()ã¯æ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆã‚’æœŸå¾…ã™ã‚‹
                parser.parse(response_content.splitlines())
                self.robot_parsers[base_url] = parser
                return parser
            else: # response_content ãŒNoneã®ã¾ã¾ã®å ´åˆï¼ˆä¸Šè¨˜ã§NoneãŒè¿”ã•ã‚ŒãŸå ´åˆï¼‰
                # ã“ã®ã‚±ãƒ¼ã‚¹ã¯ä¸Šã®status_code != 200 ã‚„ exception fetch ã§ã‚«ãƒãƒ¼ã•ã‚Œã‚‹ã¯ãš
                self.robot_parsers[base_url] = None
                return None
                
        except Exception as e: # parseæ™‚ã®äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼
            print(f"Error parsing robots.txt content from {robots_url}: {e}. Assuming allowed.")
            self.robot_parsers[base_url] = None 
            return None

    async def _can_fetch(self, url: str, user_agent: str) -> bool:
        """æŒ‡å®šã•ã‚ŒãŸURLã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¦ã‚ˆã„ã‹robots.txtã«åŸºã¥ã„ã¦åˆ¤æ–­ã™ã‚‹"""
        try:
            parsed_url = urlparse(url)
            if not parsed_url.scheme or not parsed_url.netloc:
                print(f"Invalid URL for robots.txt check: {url}. Assuming not allowed.")
                return False
            base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
            
            parser = await self._get_robot_parser(base_url)
            if parser:
                return parser.can_fetch(user_agent, url)
            
            print(f"No valid robots.txt parser for {base_url}, assuming allowed for {url} (default policy).")
            return True # robots.txtãŒãªã„ã€ã¾ãŸã¯å–å¾—/ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒãƒªã‚·ãƒ¼
        except Exception as e:
            print(f"Error in _can_fetch for URL {url}: {e}. Assuming not allowed for safety.")
            return False


    async def _scrape_articles(self, search_results: Dict[str, Any], num_articles: int) -> List[ScrapedArticle]:
        """
        æ¤œç´¢çµæœã‹ã‚‰URLã‚’æŠ½å‡ºã—ã¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° (robots.txtå¯¾å¿œ)
        """
        scraped_articles: List[ScrapedArticle] = [] 
        
        # related_questionsã‹ã‚‰URLã‚’å–å¾—ã—ã¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        related_questions = search_results.get("related_questions", [])
        for i, question_data in enumerate(related_questions[:2]):  # æœ€å¤§2ä»¶
            if len(scraped_articles) >= num_articles:
                break
            
            url = question_data.get("link")
            if url:
                # â˜… robots.txt ãƒã‚§ãƒƒã‚¯è¿½åŠ 
                if not await self._can_fetch(url, self.USER_AGENT):
                    print(f"Skipping (robots.txt): {url}")
                    continue
                try:
                    print(f"Scraping related question URL: {url}")
                    article_data = await self._scrape_url_real(url)
                    if article_data:
                        scraped_articles.append(ScrapedArticle(
                            url=url,
                            title=article_data.get("title", question_data.get("title", f"é–¢é€£è³ªå•è¨˜äº‹ {i+1}")),
                            headings=article_data.get("headings", []),
                            content=article_data.get("content", ""),
                            char_count=article_data.get("char_count", 0),
                            image_count=article_data.get("image_count", 0),
                            source_type="related_question",
                            question=question_data.get("question"),
                            video_count=article_data.get("video_count", 0),
                            table_count=article_data.get("table_count", 0),
                            list_item_count=article_data.get("list_item_count", 0),
                            external_link_count=article_data.get("external_link_count", 0),
                            internal_link_count=article_data.get("internal_link_count", 0),
                            author_info=article_data.get("author_info"),
                            publish_date=article_data.get("publish_date"),
                            modified_date=article_data.get("modified_date"),
                            schema_types=article_data.get("schema_types", [])
                        ))
                    await asyncio.sleep(settings.scraping_delay) # â˜… è¿½åŠ : ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¾Œã«é…å»¶
                except Exception as e:
                    print(f"é–¢é€£è³ªå•è¨˜äº‹ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ {url}: {e}")
                    await asyncio.sleep(settings.scraping_delay) # â˜… è¿½åŠ : ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚é…å»¶ï¼ˆæ¬¡ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ãŸã‚ï¼‰
                    continue # æ¬¡ã®URLã¸
        
        # organic_resultsã‹ã‚‰URLã‚’å–å¾—ã—ã¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        organic_results = search_results.get("organic_results", [])
        processed_organic_urls = set() # åŒã˜URLã‚’è¤‡æ•°å›å‡¦ç†ã—ãªã„ã‚ˆã†ã«

        for result in organic_results: # ã¾ãšã¯num_articlesã®åˆ¶é™ãªã—ã«ãƒ«ãƒ¼ãƒ—
            if len(scraped_articles) >= num_articles:
                 break # å¿…è¦ãªè¨˜äº‹æ•°ãŒé›†ã¾ã£ãŸã‚‰çµ‚äº†
                
            url = result.get("link")
            if url and url not in processed_organic_urls:
                processed_organic_urls.add(url) #å‡¦ç†æ¸ˆã¿ã¨ã—ã¦è¨˜éŒ²
                
                if not await self._can_fetch(url, self.USER_AGENT): # â˜… robots.txt ãƒã‚§ãƒƒã‚¯è¿½åŠ 
                    print(f"Skipping (robots.txt): {url}")
                    continue
                try:
                    print(f"Scraping organic result URL: {url}")
                    article_data = await self._scrape_url_real(url)
                    if article_data:
                        scraped_articles.append(ScrapedArticle(
                            url=url, # ã‚ªãƒªã‚¸ãƒŠãƒ«ã®URLã‚’ä¿å­˜
                            title=article_data.get("title", result.get("title", "å–å¾—ã—ãŸè¨˜äº‹")),
                            headings=article_data.get("headings", []),
                            content=article_data.get("content", ""),
                            char_count=article_data.get("char_count", 0),
                            image_count=article_data.get("image_count", 0),
                            source_type="organic_result",
                            position=result.get("position"),
                            video_count=article_data.get("video_count", 0),
                            table_count=article_data.get("table_count", 0),
                            list_item_count=article_data.get("list_item_count", 0),
                            external_link_count=article_data.get("external_link_count", 0),
                            internal_link_count=article_data.get("internal_link_count", 0),
                            author_info=article_data.get("author_info"),
                            publish_date=article_data.get("publish_date"),
                            modified_date=article_data.get("modified_date"),
                            schema_types=article_data.get("schema_types", [])
                        ))
                    await asyncio.sleep(settings.scraping_delay) # â˜… è¿½åŠ : ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¾Œã«é…å»¶
                except Exception as e:
                    print(f"è¨˜äº‹ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ {url}: {e}")
                    await asyncio.sleep(settings.scraping_delay) # â˜… è¿½åŠ : ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚é…å»¶ï¼ˆæ¬¡ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ãŸã‚ï¼‰
                    continue # æ¬¡ã®URLã¸
        
        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§ããŸè¨˜äº‹ãŒå°‘ãªã„å ´åˆã€ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã§è£œå®Œ (è¦ä»¶ã«å¿œã˜ã¦å‰Šé™¤/å¤‰æ›´)
        if len(scraped_articles) < num_articles:
            print(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°è¨˜äº‹æ•°ãŒä¸è¶³ ({len(scraped_articles)}/{num_articles})ã€‚ãƒ¢ãƒƒã‚¯è£œå®Œã¯è¡Œã„ã¾ã›ã‚“ã€‚")
            # mock_needed = num_articles - len(scraped_articles)
            # mock_articles = self._get_mock_scraped_articles(search_results, mock_needed)
            # scraped_articles.extend(mock_articles)
            pass

        return scraped_articles[:num_articles] # æœ€çµ‚çš„ã«num_articlesã«åˆ‡ã‚Šè©°ã‚ã‚‹
    
    # å®Ÿéš›ã®SerpAPIå‘¼ã³å‡ºã—ç”¨ã®é–¢æ•°ï¼ˆå¾Œã§å®Ÿè£…ï¼‰
    async def _call_serpapi_real(self, query: str) -> Dict[str, Any]:
        """
        å®Ÿéš›ã®SerpAPIå‘¼ã³å‡ºã—ï¼ˆå¾Œã§å®Ÿè£…äºˆå®šï¼‰
        """
        self._ensure_api_key()
        params = {
            "api_key": self.api_key,
            "engine": "google",
            "q": query,
            "location": "Japan",
            "google_domain": "google.com",
            "gl": "jp",
            "hl": "ja",
            "device": "desktop"
        }
        
        print(f"Calling SerpAPI with query: {query}, API Key: {'*' * (len(str(settings.serpapi_key)) - 4) + str(settings.serpapi_key)[-4:] if settings.serpapi_key else 'NOT_SET'}") # APIã‚­ãƒ¼ã®ä¸€éƒ¨ã®ã¿è¡¨ç¤º

        try:
            # SerpAPIã®Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒª `serpapi` ã¯åŸºæœ¬çš„ã«åŒæœŸçš„ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚
            # éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ (async def) å†…ã§ãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ã™ã‚‹åŒæœŸå‡¦ç†ã‚’å‘¼ã³å‡ºã™å ´åˆã€
            # ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã—ãªã„ã‚ˆã†ã« `asyncio.to_thread` ã‚’ä½¿ã†ã®ãŒä¸€èˆ¬çš„ã§ã™ã€‚
            loop = asyncio.get_running_loop()
            search = GoogleSearch(params) # åŒæœŸçš„ãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
            
            # search.get_dict() ã‚‚åŒæœŸçš„ãªãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°å‡¦ç†
            results = await loop.run_in_executor(None, search.get_dict)
            
            # print(f"SerpAPI Result for '{query}': {results}") # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼ˆçµæœãŒå¤§ãã„ã®ã§æ³¨æ„ï¼‰
            if not results or "error" in results:
                error_message = results.get("error", "Unknown SerpAPI error") if results else "Empty response from SerpAPI"
                print(f"SerpAPI returned an error or empty response: {error_message}")
                # ã‚¨ãƒ©ãƒ¼å†…å®¹ã«å¿œã˜ã¦é©åˆ‡ãªä¾‹å¤–ã‚’ç™ºç”Ÿã•ã›ã‚‹ã‹ã€ã‚¨ãƒ©ãƒ¼ã‚’ç¤ºã™æƒ…å ±ã‚’è¿”ã™
                # ä¾‹: raise SerpAPIError(error_message)
                return {"error": error_message, "search_parameters": results.get("search_parameters")}

            return results

        except Exception as e:
            print(f"Exception during SerpAPI call for query '{query}': {e}")
            # ã“ã“ã§ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã‚„äºˆæœŸã›ã¬ä¾‹å¤–ã‚’æ•æ‰
            # ä¾‹: raise NetworkError(f"Failed to call SerpAPI: {e}")
            return {"error": str(e), "query_params": params} # ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’å«ã‚“ã dictã‚’è¿”ã™ä¾‹
    
    async def _classify_headings_semantically(self, structured_headings: List[Dict[str, Any]], original_url: str = "N/A") -> List[Dict[str, Any]]:
        """
        è¦‹å‡ºã—ãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚Šã€å„è¦‹å‡ºã—ã«æ„å‘³çš„ãªåˆ†é¡ã‚’è¡Œã† (ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹)ã€‚
        original_url ã¯APIãƒ™ãƒ¼ã‚¹ã®åˆ†é¡å™¨ã¨ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹äº’æ›æ€§ã®ãŸã‚ã«è¿½åŠ ã•ã‚Œã¾ã—ãŸãŒã€ã“ã®é–¢æ•°ã§ã¯ä½¿ç”¨ã•ã‚Œã¾ã›ã‚“ã€‚
        """
        classified_headings = []
        for heading_node in structured_headings:
            new_node = heading_node.copy()
            level = new_node.get("level", 0)
            text = new_node.get("text", "")
            semantic_type = "body"
            lower_text = text.lower()

            if level <= 2:
                if any(kw in lower_text for kw in ["ã¯ã˜ã‚ã«", "åºè«–", "å°å…¥", "introduction"]):
                    semantic_type = "introduction"
                elif any(kw in lower_text for kw in ["ã¾ã¨ã‚", "çµè«–", "çµè«–ã¨ã—ã¦", "conclusion", "ãŠã‚ã‚Šã«"]):
                    semantic_type = "conclusion"
            
            new_node["semantic_type"] = semantic_type
            
            if "children" in new_node and new_node["children"]:
                new_node["children"] = await self._classify_headings_semantically(new_node["children"], original_url)
            
            classified_headings.append(new_node)
        return classified_headings
    
    async def _classify_headings_semantically_gemini(self, structured_headings: List[Dict[str, Any]], original_url: str = "N/A") -> List[Dict[str, Any]]:
        """Gemini API ã‚’ä½¿ç”¨ã—ã¦è¦‹å‡ºã—ãƒªã‚¹ãƒˆã‚’æ„å‘³çš„ã«åˆ†é¡ã™ã‚‹ã€‚"""
        if not settings.gemini_api_key:
            print("Gemini APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®åˆ†é¡ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚‚ç¾çŠ¶ã‚ã‚Šã¾ã›ã‚“ã€‚")
            for heading in structured_headings:
                heading['semantic_type'] = 'body' 
                if heading.get('children'):
                    # å†å¸°å‘¼ã³å‡ºã—ã«ã‚‚ original_url ã‚’æ¸¡ã™
                    await self._classify_headings_semantically_gemini(heading['children'], original_url)
            return structured_headings

        try:
            setup_genai_client()
        except Exception as e:
            print(f"Gemini APIã‚­ãƒ¼ã®è¨­å®šã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            for heading in structured_headings: # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                heading['semantic_type'] = 'body'
                if heading.get('children'):
                     # å†å¸°å‘¼ã³å‡ºã—ã«ã‚‚ original_url ã‚’æ¸¡ã™
                     await self._classify_headings_semantically_gemini(heading['children'], original_url)
            return structured_headings

        model = genai.GenerativeModel('gemini-2.0-flash') # ã¾ãŸã¯ 'gemini-pro'
        
        # APIã«æ¸¡ã™ãŸã‚ã«è¦‹å‡ºã—ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆã‚’æº–å‚™ (IDã‚‚æŒ¯ã‚‹)
        def _flatten_headings(headings_list, prefix=""):
            flat_list = []
            for i, heading in enumerate(headings_list):
                heading_id = f"{prefix}{i+1}"
                flat_list.append({"id": heading_id, "text": heading["text"], "level": heading["level"]})
                if heading.get("children"):
                    flat_list.extend(_flatten_headings(heading["children"], f"{heading_id}."))
            return flat_list

        flat_headings_with_ids = _flatten_headings(structured_headings)
        
        if not flat_headings_with_ids:
            return structured_headings

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æº–å‚™
        prompt_headings = [{"id": h["id"], "text": h["text"], "level": h["level"]} for h in flat_headings_with_ids]
        
        # Geminiç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ (OpenAIç‰ˆã®ã‚‚ã®ã‚’ãƒ™ãƒ¼ã‚¹ã«èª¿æ•´)
        full_prompt = (
            "ã‚ãªãŸã¯ã€ä¸ãˆã‚‰ã‚ŒãŸè¨˜äº‹ã®è¦‹å‡ºã—ãƒªã‚¹ãƒˆã‚’åˆ†æã—ã€å„è¦‹å‡ºã—ãŒè¨˜äº‹å…¨ä½“ã®æ§‹é€ ã®ä¸­ã§ã©ã®ã‚ˆã†ãªæ„å‘³çš„å½¹å‰²ã‚’æŒã¤ã‹ã‚’åˆ¤æ–­ã™ã‚‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\\n"
            "ä»¥ä¸‹ã®æŒ‡ç¤ºã«å¾“ã£ã¦ã€å„è¦‹å‡ºã—ã‚’æœ€ã‚‚é©åˆ‡ã¨æ€ã‚ã‚Œã‚‹ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡ã—ã¦ãã ã•ã„ã€‚\\n\\n"
            "ã€åˆ†é¡ã‚«ãƒ†ã‚´ãƒªã¨åˆ¤æ–­ãƒ’ãƒ³ãƒˆã€‘\\n"
            "1. 'introduction': è¨˜äº‹å…¨ä½“ã®å°å…¥éƒ¨ã€åºè«–ã€æ¦‚è¦ã€ç›®çš„ã€èƒŒæ™¯ãªã©ã‚’èª¬æ˜ã™ã‚‹è¦‹å‡ºã—ã€‚\\n"
            "   - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¾‹: ã€Œã¯ã˜ã‚ã«ã€ã€Œåºè«–ã€ã€Œå°å…¥ã€ã€Œæ¦‚è¦ã€ã€Œã“ã®è¨˜äº‹ã«ã¤ã„ã¦ã€ã€Œç›®çš„ã€ã€ŒèƒŒæ™¯ã€ã€Œï½ã¨ã¯ï¼Ÿã€\\n"
            "   - è¨˜äº‹ã®æœ€åˆã®æ–¹ï¼ˆç‰¹ã«æœ€åˆã®H1ã¾ãŸã¯H2ï¼‰ã«å‡ºç¾ã—ã‚„ã™ã„ã€‚\\n"
            "2. 'body': è¨˜äº‹ã®æœ¬è«–éƒ¨åˆ†ã€‚ä¸»é¡Œã«é–¢ã™ã‚‹å…·ä½“çš„ãªèª¬æ˜ã€è­°è«–ã€æ–¹æ³•ã€æ‰‹é †ã€äº‹ä¾‹ã€ãƒ¡ãƒªãƒƒãƒˆãƒ»ãƒ‡ãƒ¡ãƒªãƒƒãƒˆã€åˆ†æã€è€ƒå¯Ÿãªã©ã€‚\\n"
            "   - ä¸Šè¨˜ä»¥å¤–ã®ã‚«ãƒ†ã‚´ãƒªã«æ˜ç¢ºã«å½“ã¦ã¯ã¾ã‚‰ãªã„å ´åˆã¯ã€ã“ã®ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚\\n"
            "3. 'conclusion': è¨˜äº‹å…¨ä½“ã®çµè«–éƒ¨ã€ã¾ã¨ã‚ã€è¦ç´„ã€ä»Šå¾Œã®å±•æœ›ã€æè¨€ãªã©ã€‚\\n"
            "   - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¾‹: ã€Œã¾ã¨ã‚ã€ã€Œçµè«–ã€ã€Œç·æ‹¬ã€ã€ŒãŠã‚ã‚Šã«ã€ã€Œæœ€å¾Œã«ã€ã€Œä»Šå¾Œã®èª²é¡Œã€ã€Œæè¨€ã€\\n"
            "   - è¨˜äº‹ã®æœ€å¾Œã®æ–¹ã«å‡ºç¾ã—ã‚„ã™ã„ã€‚\\n"
            "4. 'faq': ã‚ˆãã‚ã‚‹è³ªå•ã¨ãã®å›ç­”ã‚’ã¾ã¨ã‚ãŸã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®è¦‹å‡ºã—ã€‚\\n"
            "   - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¾‹: ã€ŒFAQã€ã€Œã‚ˆãã‚ã‚‹è³ªå•ã€ã€ŒQ&Aã€\\n"
            "5. 'references': å‚è€ƒæ–‡çŒ®ã€å‚è€ƒè³‡æ–™ã€é–¢é€£æƒ…å ±æºãªã©ã‚’ç¤ºã™ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®è¦‹å‡ºã—ã€‚\\n"
            "   - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¾‹: ã€Œå‚è€ƒæ–‡çŒ®ã€ã€Œå‚è€ƒè³‡æ–™ã€ã€Œé–¢é€£ãƒªãƒ³ã‚¯ã€ã€Œã‚‚ã£ã¨èª­ã‚€ã€\\n"
            "6. 'other': ä¸Šè¨˜ã®ã„ãšã‚Œã«ã‚‚æ˜ç¢ºã«å½“ã¦ã¯ã¾ã‚‰ãªã„ç‰¹æ®Šãªå½¹å‰²ã‚’æŒã¤è¦‹å‡ºã—ï¼ˆä¾‹: ç”¨èªé›†ã€ä¼šç¤¾æ¦‚è¦ãªã©ï¼‰ã€‚ä½¿ç”¨ã¯æœ€å°é™ã«ã—ã¦ãã ã•ã„ã€‚\\n\\n"
            "ã€æŒ‡ç¤ºã€‘\\n"
            "- è¦‹å‡ºã—ã®ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã€ãƒ¬ãƒ™ãƒ«ï¼ˆh1-h6ï¼‰ã€ãã—ã¦ãƒªã‚¹ãƒˆå†…ã§ã®å‡ºç¾é †ã‚’è€ƒæ…®ã—ã¦åˆ†é¡ã—ã¦ãã ã•ã„ã€‚\\n"
            "- ç‰¹ã«ã€'introduction'ã¨'conclusion'ã¯è¨˜äº‹æ§‹é€ ã«ãŠã‘ã‚‹ä½ç½®ãŒé‡è¦ã§ã™ã€‚\\n"
            "- æ˜ç¢ºãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¾‹:ã€Œã¾ã¨ã‚ã€ï¼‰ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã€ãã‚Œã‚’å„ªå…ˆã—ã¦åˆ†é¡ã—ã¦ãã ã•ã„ã€‚\\n"
            "- å›ç­”ã¯ã€å„è¦‹å‡ºã—ã®IDã¨åˆ†é¡çµæœã‚’å«ã‚€JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆå½¢å¼ã§ã€ä»¥ä¸‹ã®ã‚ˆã†ã«è¿”ã—ã¦ãã ã•ã„:\\n"
            "  ä¾‹: [{'id': '1', 'classification': 'introduction'}, {'id': '1.1', 'classification': 'body'}, {'id': '2', 'classification': 'conclusion'}]\\n"
            "- ä»–ã®å½¢å¼ã§ã¯ãªãã€å¿…ãšã“ã®JSONãƒªã‚¹ãƒˆå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚\\n\\n"
            "ä»¥ä¸‹ã®è¦‹å‡ºã—ãƒªã‚¹ãƒˆã‚’åˆ†é¡ã—ã¦ãã ã•ã„:\\n"
            f"{json.dumps(prompt_headings, ensure_ascii=False, indent=2)}"
        )

        print(f"Gemini APIã‚’å‘¼ã³å‡ºã—ã¾ã™ (URL: {original_url}, è¦‹å‡ºã—æ•°: {len(flat_headings_with_ids)})...")
        start_time = time.monotonic()

        try:
            generation_config = genai.types.GenerationConfig(
                # candidate_count=1, # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1
                response_mime_type="application/json", 
                temperature=0.2
            )
            response = await model.generate_content_async(
                contents=[full_prompt], 
                generation_config=generation_config
            )
            
            raw_response_content = response.text 
            
            classification_map: Dict[str, str] = {}

            if raw_response_content:
                try:
                    json_data = json.loads(raw_response_content)
                    if isinstance(json_data, list):
                        for item in json_data:
                            if isinstance(item, dict) and "id" in item and "classification" in item:
                                classification_map[item["id"]] = item["classification"] 
                    elif isinstance(json_data, dict):
                        if "classifications" in json_data and isinstance(json_data["classifications"], list):
                            for item in json_data["classifications"]:
                                if isinstance(item, dict) and "id" in item and "classification" in item:
                                    classification_map[item["id"]] = item["classification"] 
                        else: # ãƒ•ãƒ©ãƒƒãƒˆãªè¾æ›¸ã¾ãŸã¯ãã®ä»–ã®è¾æ›¸å½¢å¼
                            for key, value in json_data.items():
                                if isinstance(value, str): # { "id": "class" }
                                    classification_map[key] = value
                                elif isinstance(value, dict) and "classification" in value: # { "id": {"classification": "class"} }
                                    classification_map[key] = value["classification"]
                            if not classification_map: # ä¸Šè¨˜ã§è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆ
                                print(f"Gemini APIã‹ã‚‰ã®è¾æ›¸å½¢å¼JSONã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•— (URL: {original_url}): {raw_response_content[:200]}...")
                    else:
                        print(f"Gemini APIã‹ã‚‰ã®äºˆæœŸã—ãªã„JSONãƒ«ãƒ¼ãƒˆå‹ (URL: {original_url}): {type(json_data)} {raw_response_content[:200]}...")

                except json.JSONDecodeError:
                    print(f"Gemini APIã‹ã‚‰ã®JSONãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ (URL: {original_url}): {raw_response_content[:200]}...")
            else:
                print(f"Gemini APIã‹ã‚‰ã®å¿œç­”ãŒç©ºã§ã—ãŸ (URL: {original_url})")
            
            end_time = time.monotonic()
            print(f"Gemini API å‡¦ç†æ™‚é–“: {end_time - start_time:.2f}ç§’ (URL: {original_url})")

            def _apply_classification(headings_list, prefix=""):
                for i, heading in enumerate(headings_list):
                    heading_id = f"{prefix}{i+1}"
                    heading["semantic_type"] = classification_map.get(heading_id, "body") 
                    if heading.get("children"):
                        _apply_classification(heading["children"], f"{heading_id}.")
                return headings_list
            
            return _apply_classification(structured_headings)

        except Exception as e:
            print(f"Gemini APIå‘¼ã³å‡ºã—ä¸­ã¾ãŸã¯å¿œç­”å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ (URL: {original_url}): {e}")

        # ã‚¨ãƒ©ãƒ¼æ™‚ã‚„APIã‚­ãƒ¼ãŒãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        for heading_node in structured_headings:
            heading_node['semantic_type'] = 'body' 
            if 'children' in heading_node and heading_node['children']:
                 await self._classify_headings_semantically_gemini(heading_node['children'], original_url) # å†å¸°å‘¼ã³å‡ºã—
        return structured_headings

    def _add_char_counts_to_headings_recursive(
        self,
        heading_node_list: List[Dict[str, Any]], 
        all_heading_tags_in_document: List[Any] # Flat list of all H tags in the order they appear
    ):
        """
        è¦‹å‡ºã—ãƒãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆã«ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ–‡å­—æ•°ã‚’å†å¸°çš„ã«è¿½åŠ ã™ã‚‹ã€‚
        'char_count_section' ã¯ã€ãã®è¦‹å‡ºã—ã‹ã‚‰æ¬¡ã®åŒä½ã¾ãŸã¯ä¸Šä½ã®è¦‹å‡ºã—ã¾ã§ã®
        é–“ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆä¸‹ä½è¦‹å‡ºã—ã®ãƒ†ã‚­ã‚¹ãƒˆã‚‚å«ã‚€grossã‚«ã‚¦ãƒ³ãƒˆï¼‰ã®æ–‡å­—æ•°ã€‚
        """
        tag_to_document_index_map = {tag: i for i, tag in enumerate(all_heading_tags_in_document)}

        for node in heading_node_list:
            if 'tag' not in node or not hasattr(node['tag'], 'name'):
                node['char_count_section'] = 0
                if node.get('children'):
                    self._add_char_counts_to_headings_recursive(node['children'], all_heading_tags_in_document)
                continue

            current_tag = node['tag']
            current_tag_doc_index = tag_to_document_index_map.get(current_tag)

            section_limit_tag = None
            if current_tag_doc_index is not None:
                for i in range(current_tag_doc_index + 1, len(all_heading_tags_in_document)):
                    potential_next_tag = all_heading_tags_in_document[i]
                    if int(potential_next_tag.name[1:]) <= node['level']:
                        section_limit_tag = potential_next_tag
                        break
            
            text_content_parts = []
            for sibling_element in current_tag.next_siblings:
                if section_limit_tag and sibling_element == section_limit_tag:
                    break 
                
                if hasattr(sibling_element, 'name') and sibling_element.name in ['script', 'style', 'nav', 'header', 'footer', 'aside', 'form']:
                    continue
                
                if isinstance(sibling_element, NavigableString):
                    stripped_text = str(sibling_element).strip()
                    if stripped_text:
                        text_content_parts.append(stripped_text)
                elif hasattr(sibling_element, 'get_text'):
                    stripped_text = sibling_element.get_text(separator=' ', strip=True)
                    if stripped_text:
                        text_content_parts.append(stripped_text)
            
            full_section_text = " ".join(text_content_parts)
            # æ–‡å­—æ•°ã¯ã‚¹ãƒšãƒ¼ã‚¹ã‚’é™¤ã„ãŸã‚‚ã®ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆä»»æ„ã€ä¸€è²«æ€§ã®ãŸã‚ï¼‰
            node['char_count_section'] = len(full_section_text.replace(" ", ""))

            if node.get('children'):
                self._add_char_counts_to_headings_recursive(node['children'], all_heading_tags_in_document)


    async def _scrape_url_real(self, url: str) -> Optional[Dict[str, Any]]:
        """ å®Ÿéš›ã®URLã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° """
        current_url = url
        try:
            headers = {'User-Agent': self.USER_AGENT}
            response = await asyncio.to_thread(
                lambda: requests.get(current_url, headers=headers, timeout=settings.scraping_timeout, allow_redirects=True)
            )
            
            if response.history:
                current_url = response.url
            if response.status_code != 200:
                return None
            
            response.encoding = response.apparent_encoding
            soup = BeautifulSoup(response.text, 'html.parser')
            title_tag = soup.find('title')
            title = title_tag.get_text(strip=True) if title_tag else "ã‚¿ã‚¤ãƒˆãƒ«å–å¾—ã§ããš"
            
            main_content_selectors = [
                'article', 'main',
                'div[class*="content"]', 'div[class*="post"]', 'div[class*="entry"]', 'div[class*="article"]',
                'section[class*="content"]', 'section[class*="post"]', 'section[class*="entry"]',
                'div[id*="content"]', 'div[id*="main"]',
            ]
            content_element = None
            for selector in main_content_selectors:
                content_element = soup.select_one(selector)
                if content_element:
                    break
            
            if not content_element:
                content_element = soup.body
            if not content_element:
                return {"title": title, "headings": [], "content": "", "char_count": 0, "image_count": 0}

            # ä¸è¦è¦ç´ ã®é™¤å» (æ–‡å­—æ•°ã‚«ã‚¦ãƒ³ãƒˆå‰ã«å®Ÿè¡Œ)
            for unwanted_selector in ['nav', 'footer', 'header', 'aside', 'form', 'script', 'style', '.noprint', '[aria-hidden="true"]', 'figure > figcaption']:
                for tag in content_element.select(unwanted_selector):
                    tag.decompose()
            for ad_selector in ['div[class*="ad"]', 'div[id*="ad"]', 'div[class*="OUTBRAIN"]', 'div[class*="recommend"]', 'aside[class*="related"]']:
                for tag in content_element.select(ad_selector):
                    tag.decompose()

            # è¦‹å‡ºã—ã‚¿ã‚°ã®æŠ½å‡º (é™¤å»å‡¦ç†å¾Œã«è¡Œã†)
            all_heading_tags_in_content_element = content_element.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
            
            structured_headings: List[Dict[str, Any]] = []
            parent_stack: List[Tuple[int, List[Dict[str, Any]]]] = [(0, structured_headings)]

            for tag_object in all_heading_tags_in_content_element: # tag_object ã‚’ç›´æ¥ä½¿ç”¨
                if not hasattr(tag_object, 'name') or not tag_object.name:
                    continue
                level = int(tag_object.name[1:])
                text = tag_object.get_text(strip=True)
                if not text or len(text) >= 200:
                    continue

                current_heading_node = {"level": level, "text": text, "children": [], "tag": tag_object} # â˜… tag ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä¿å­˜

                while parent_stack[-1][0] >= level:
                    parent_stack.pop()
                parent_stack[-1][1].append(current_heading_node)
                children_list: List[Dict[str, Any]] = current_heading_node["children"]
                parent_stack.append((level, children_list))
            
            # æ„å‘³çš„åˆ†é¡
            classified_final_headings = await self._classify_headings_semantically(structured_headings, original_url=current_url)
            
            # â˜… ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ–‡å­—æ•°ã‚«ã‚¦ãƒ³ãƒˆã®è¿½åŠ 
            if classified_final_headings and all_heading_tags_in_content_element:
                self._add_char_counts_to_headings_recursive(classified_final_headings, all_heading_tags_in_content_element)

            # è¨˜äº‹å…¨ä½“ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã¨æ–‡å­—æ•°ã‚«ã‚¦ãƒ³ãƒˆ (ã“ã‚Œã¯å¤‰æ›´ãªã—)
            text_blocks = []
            for element in content_element.find_all(['p', 'div', 'li', 'span', 'td', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'], recursive=True):
                if not hasattr(element, 'find_all'):
                    continue
                # ã‚¹ã‚¯ãƒªãƒ—ãƒˆ/ã‚¹ã‚¿ã‚¤ãƒ«ã¯æ—¢ã«é™¤å»ã•ã‚Œã¦ã„ã‚‹ã¯ãšã ãŒå¿µã®ãŸã‚
                for unwanted_tag in element.find_all(['script', 'style'], recursive=False):
                    unwanted_tag.decompose()
                block_text = element.get_text(separator=' ', strip=True)
                if block_text and len(block_text) > 20:
                    parent_text = element.parent.get_text(separator=' ', strip=True) if element.parent else ""
                    if parent_text != block_text or (hasattr(element, 'name') and element.name in ['p', 'li', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
                        text_blocks.append(block_text)
            
            final_content_parts = []
            seen_content_parts = set()
            for block in text_blocks:
                first_part = block[:100]
                if first_part not in seen_content_parts:
                    final_content_parts.append(block)
                    seen_content_parts.add(first_part)
                    if sum(len(p) for p in final_content_parts) > 15000:
                        break
            
            content_text = "\n\n".join(final_content_parts)
            char_count = len("".join(final_content_parts).replace(" ","")) # ã‚¹ãƒšãƒ¼ã‚¹é™¤å¤–ã§çµ±ä¸€
            
            img_tags = content_element.find_all('img')
            image_count = len([img for img in img_tags if hasattr(img, 'get') and img.get('src') and isinstance(img.get('src'), str) and not img.get('src', '').startswith('data:')])
            
            # â˜… æ–°ã—ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆåˆ†æ
            # å‹•ç”»æ•°ã®è¨ˆç®—ï¼ˆvideo + YouTubeãªã©ã®iframeï¼‰
            video_tags = content_element.find_all('video')
            iframe_tags = content_element.find_all('iframe')
            video_iframes = [iframe for iframe in iframe_tags 
                           if hasattr(iframe, 'get') and iframe.get('src') and isinstance(iframe.get('src'), str) and any(domain in iframe.get('src', '') 
                               for domain in ['youtube.com', 'youtu.be', 'vimeo.com', 'dailymotion.com'])]
            video_count = len(video_tags) + len(video_iframes)
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«æ•°
            table_count = len(content_element.find_all('table'))
            
            # ãƒªã‚¹ãƒˆé …ç›®ç·æ•°
            list_items = content_element.find_all('li')
            list_item_count = len(list_items)
            
            # ãƒªãƒ³ã‚¯åˆ†æï¼ˆå¤–éƒ¨ãƒ»å†…éƒ¨ï¼‰
            all_links = content_element.find_all('a', href=True)
            external_links = []
            internal_links = []
            current_domain = urlparse(current_url).netloc if current_url else ""
            
            for link in all_links:
                if not hasattr(link, 'get'):
                    continue
                href = link.get('href', '')
                if not isinstance(href, str):
                    continue
                if href.startswith('http'):
                    link_domain = urlparse(href).netloc
                    if link_domain != current_domain:
                        external_links.append(href)
                    else:
                        internal_links.append(href)
                elif href.startswith('/') or not href.startswith('#'):
                    internal_links.append(href)  # ç›¸å¯¾ãƒ‘ã‚¹ã¯å†…éƒ¨ãƒªãƒ³ã‚¯ã¨ã¿ãªã™
            
            external_link_count = len(external_links)
            internal_link_count = len(internal_links)
            
            # â˜… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºï¼ˆåŸºæœ¬ç‰ˆï¼‰
            author_info = None
            publish_date = None
            modified_date = None
            
            # è‘—è€…æƒ…å ±ã®æŠ½å‡º
            author_meta = soup.find('meta', attrs={'name': 'author'})
            if author_meta and hasattr(author_meta, 'get'):
                author_content = author_meta.get('content')
                if isinstance(author_content, str):
                    author_info = author_content
            else:
                # ã‚¯ãƒ©ã‚¹åã§è‘—è€…æƒ…å ±ã‚’æ¢ã™
                author_elements = soup.find_all(['div', 'span', 'p'], class_=re.compile(r'author', re.I))
                if author_elements:
                    author_info = author_elements[0].get_text(strip=True)
            
            # å…¬é–‹æ—¥ãƒ»æ›´æ–°æ—¥ã®æŠ½å‡º
            pub_meta = soup.find('meta', attrs={'property': 'article:published_time'})
            if pub_meta and hasattr(pub_meta, 'get'):
                pub_content = pub_meta.get('content')
                if isinstance(pub_content, str):
                    publish_date = pub_content
            
            mod_meta = soup.find('meta', attrs={'property': 'article:modified_time'})
            if mod_meta and hasattr(mod_meta, 'get'):
                mod_content = mod_meta.get('content')
                if isinstance(mod_content, str):
                    modified_date = mod_content
            
            # â˜… æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ï¼ˆSchema.orgï¼‰ã®æŠ½å‡º
            schema_types = []
            ld_json_scripts = soup.find_all('script', type='application/ld+json')
            for script in ld_json_scripts:
                try:
                    if hasattr(script, 'string') and script.string and isinstance(script.string, str):
                        ld_data = json.loads(script.string)
                        if isinstance(ld_data, dict) and '@type' in ld_data:
                            schema_types.append(ld_data['@type'])
                        elif isinstance(ld_data, list):
                            for item in ld_data:
                                if isinstance(item, dict) and '@type' in item:
                                    schema_types.append(item['@type'])
                except (json.JSONDecodeError, AttributeError, TypeError):
                    continue
            
            return {
                "title": title,
                "headings": classified_final_headings[:30],
                "content": content_text.strip()[:10000],
                "char_count": char_count,
                "image_count": image_count,
                # â˜… æ–°ã—ã„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
                "video_count": video_count,
                "table_count": table_count,
                "list_item_count": list_item_count,
                "external_link_count": external_link_count,
                "internal_link_count": internal_link_count,
                "author_info": author_info,
                "publish_date": publish_date,
                "modified_date": modified_date,
                "schema_types": schema_types
            }
            
        except requests.exceptions.Timeout:
            print(f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {url}")
            return None
        except requests.exceptions.RequestException as e:
            print(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼ {url}: {e}")
            return None
        except Exception as e:
            print(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸€èˆ¬ã‚¨ãƒ©ãƒ¼ {url}: {e}")
            import traceback
            traceback.print_exc()
            return None

# ã‚µãƒ¼ãƒ“ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆé…å»¶ãƒ­ãƒ¼ãƒ‰ï¼‰
_serpapi_service_instance = None

def get_serpapi_service() -> SerpAPIService:
    """SerpAPIServiceã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _serpapi_service_instance
    if _serpapi_service_instance is None:
        _serpapi_service_instance = SerpAPIService()
    return _serpapi_service_instance

# ä½¿ç”¨æ™‚ã¯ get_serpapi_service() ã‚’å‘¼ã³å‡ºã—ã¦ãã ã•ã„ 

# serpapi_service.py ã®æœ«å°¾ã«è¿½åŠ  (ãƒ†ã‚¹ãƒˆå¾Œå‰Šé™¤ã¾ãŸã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ)
if __name__ == '__main__':
    import asyncio
    import json 

    async def main_test_call_real():
        service = SerpAPIService()
        test_query = "Python FastAPI" 
        print(f"--- Testing _call_serpapi_real for query: '{test_query}' ---")
        try:
            result = await service._call_serpapi_real(query=test_query)
            print("--- Full API Response (JSON) ---")
            print(json.dumps(result, indent=2, ensure_ascii=False))
            print("--- End of Full API Response ---")
            if "error" in result:
                print(f"Test FAILED or API returned error: {result['error']}")
            else:
                print("Test SUCCEEDED. Basic API call seems to work.")
        except Exception as e:
            print(f"Test FAILED with unhandled exception: {e}")
        print("--- End of _call_serpapi_real test ---")

    async def main_test_scrape_real_serp(use_generative_ai_for_classification: bool = False): 
        service = SerpAPIService()
        test_query = "çŠ¬ ãƒšãƒƒãƒˆ ãƒšãƒƒãƒˆã‚·ãƒ§ãƒƒãƒ— ãƒšãƒƒãƒˆç”¨å“" 
        num_to_scrape = 3 # â˜… ãƒ†ã‚¹ãƒˆã®ãŸã‚è¨˜äº‹æ•°ã‚’å°‘ãªãç¶­æŒ (å…ƒã¯3)
        print(f"\n--- Testing full analyze_keywords for query: '{test_query}' with {num_to_scrape} articles to scrape ---")
        
        # Temporarily override classification method for testing
        if use_generative_ai_for_classification:
            print("--- è¦‹å‡ºã—åˆ†é¡ã« Gemini API ã‚’ä½¿ç”¨ã—ã¾ã™ ---")
            # Use Gemini classification directly in the service call
        else:
            print("--- è¦‹å‡ºã—åˆ†é¡ã«ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ç”¨ã—ã¾ã™ ---")
        
        analysis_result = await service.analyze_keywords(keywords=[test_query], num_articles_to_scrape=num_to_scrape)
        
        print("\n--- SerpAnalysisResult (from analyze_keywords) ---")
        print(f"Search Query: {analysis_result.search_query}")
        print(f"Total Results from SerpAPI: {analysis_result.total_results}")
        print(f"Number of Related Questions from SerpAPI: {len(analysis_result.related_questions)}")
        print(f"Number of Organic Results from SerpAPI: {len(analysis_result.organic_results)}")
        print(f"Number of Scraped Articles: {len(analysis_result.scraped_articles)}")
        print(f"Average Char Count of Scraped: {analysis_result.average_char_count}")
        print(f"Suggested Target Length: {analysis_result.suggested_target_length}")
        
        # â˜… æ–°ã—ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆåˆ†æã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
        if analysis_result.scraped_articles:
            total_videos = sum(getattr(article, 'video_count', 0) for article in analysis_result.scraped_articles)
            total_tables = sum(getattr(article, 'table_count', 0) for article in analysis_result.scraped_articles)
            total_list_items = sum(getattr(article, 'list_item_count', 0) for article in analysis_result.scraped_articles)
            total_external_links = sum(getattr(article, 'external_link_count', 0) for article in analysis_result.scraped_articles)
            total_internal_links = sum(getattr(article, 'internal_link_count', 0) for article in analysis_result.scraped_articles)
            
            articles_with_author = sum(1 for article in analysis_result.scraped_articles if getattr(article, 'author_info', None))
            articles_with_publish_date = sum(1 for article in analysis_result.scraped_articles if getattr(article, 'publish_date', None))
            articles_with_schema = sum(1 for article in analysis_result.scraped_articles if getattr(article, 'schema_types', []))
            
            print("")
            print("ğŸ“Š Content Format Analysis Summary:")
            print(f"  Total Videos: {total_videos} (avg: {total_videos/len(analysis_result.scraped_articles):.1f} per article)")
            print(f"  Total Tables: {total_tables} (avg: {total_tables/len(analysis_result.scraped_articles):.1f} per article)")
            print(f"  Total List Items: {total_list_items} (avg: {total_list_items/len(analysis_result.scraped_articles):.1f} per article)")
            print(f"  Total External Links: {total_external_links} (avg: {total_external_links/len(analysis_result.scraped_articles):.1f} per article)")
            print(f"  Total Internal Links: {total_internal_links} (avg: {total_internal_links/len(analysis_result.scraped_articles):.1f} per article)")
            print("")
            print("ğŸ” E-E-A-T Metadata Analysis:")
            print(f"  Articles with Author Info: {articles_with_author}/{len(analysis_result.scraped_articles)} ({articles_with_author/len(analysis_result.scraped_articles)*100:.1f}%)")
            print(f"  Articles with Publish Date: {articles_with_publish_date}/{len(analysis_result.scraped_articles)} ({articles_with_publish_date/len(analysis_result.scraped_articles)*100:.1f}%)")
            print(f"  Articles with Schema Data: {articles_with_schema}/{len(analysis_result.scraped_articles)} ({articles_with_schema/len(analysis_result.scraped_articles)*100:.1f}%)")

            print("\n--- Details of Scraped Articles ---")
            if not analysis_result.scraped_articles: 
                print("No articles were scraped.")
            else:
                # â˜… é‡è¤‡ãƒã‚§ãƒƒã‚¯è¿½åŠ 
                urls_seen = set()
                for i, article in enumerate(analysis_result.scraped_articles):
                    print(f"--- Article {i+1} ---")
                    print(f"  URL: {article.url}")
                    
                    # â˜… é‡è¤‡URLãƒã‚§ãƒƒã‚¯
                    if article.url in urls_seen:
                        print("  âš ï¸  WARNING: Duplicate URL detected!")
                    else:
                        urls_seen.add(article.url)
                    
                    print(f"  Title: {article.title}")
                    print(f"  Headings Count: {len(article.headings)}")
                    print("  Headings Structure:")
                    
                    # â˜… JSONå‡ºåŠ›ã‚’å®‰å…¨ã«å®Ÿè¡Œ
                    try:
                        def format_headings_for_print(headings_list):
                            formatted = []
                            for h_dict in headings_list: # Renamed h to h_dict to avoid conflict
                                entry = {
                                    "level": h_dict.get("level"), 
                                    "text": h_dict.get("text"), 
                                    "semantic_type": h_dict.get("semantic_type", "N/A"),
                                    "char_count_section": h_dict.get("char_count_section", "N/A") # â˜… è¿½åŠ 
                                }
                                if h_dict.get("children"):
                                    entry["children"] = format_headings_for_print(h_dict["children"])
                                formatted.append(entry)
                            return formatted
                            
                        formatted_headings = format_headings_for_print(article.headings)
                        print(json.dumps(formatted_headings, indent=2, ensure_ascii=False))
                        
                    except TypeError as e:
                        print(f"    âŒ Could not serialize headings to JSON: {e}")
                        print(f"    Raw headings data (first 3): {article.headings[:3]}")
                    except Exception as e:
                        print(f"    âŒ Unexpected error in JSON serialization: {e}")

                    content_preview_text = article.content[:200].replace('\n', ' ')
                    print(f"  Content Preview: {content_preview_text}...")
                    print(f"  Char Count: {article.char_count}") # Overall article char count
                    print(f"  Image Count: {article.image_count}")
                    
                    # â˜… æ–°ã—ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆåˆ†æçµæœã‚’è¡¨ç¤º
                    print(f"  Video Count: {getattr(article, 'video_count', 0)}")
                    print(f"  Table Count: {getattr(article, 'table_count', 0)}")
                    print(f"  List Item Count: {getattr(article, 'list_item_count', 0)}")
                    print(f"  External Link Count: {getattr(article, 'external_link_count', 0)}")
                    print(f"  Internal Link Count: {getattr(article, 'internal_link_count', 0)}")
                    
                    # â˜… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æƒ…å ±ã‚’è¡¨ç¤º
                    author_info = getattr(article, 'author_info', None)
                    if author_info:
                        print(f"  Author Info: {author_info}")
                    else:
                        print("  Author Info: Not found")
                    
                    publish_date = getattr(article, 'publish_date', None)
                    if publish_date:
                        print(f"  Publish Date: {publish_date}")
                    else:
                        print("  Publish Date: Not found")
                    
                    modified_date = getattr(article, 'modified_date', None)
                    if modified_date:
                        print(f"  Modified Date: {modified_date}")
                    else:
                        print("  Modified Date: Not found")
                    
                    schema_types = getattr(article, 'schema_types', [])
                    if schema_types:
                        print(f"  Schema Types: {', '.join(schema_types)}")
                    else:
                        print("  Schema Types: None found")
                    
                    print(f"  Source Type: {article.source_type}")
                    if article.position:
                        print(f"  Original Position: {article.position}")
                    if article.question:
                        print(f"  Original Question: {article.question}")
                
        print("--- End of full analyze_keywords test ---")

    # asyncio.run(main_test_call_real())
    # asyncio.run(main_test_scrape_real_serp(use_generative_ai_for_classification=False)) 
    asyncio.run(main_test_scrape_real_serp(use_generative_ai_for_classification=True)) 